{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection in Twitter Data\n",
    "The scope of this notebook is to implement a Naive-Bayes classifier that can be used to predict emotion in Twitter messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the location of the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "dataDir = \"Data/ssec-aggregated/\"\n",
    "trainFile = dataDir+\"train-combined-0.0.csv\"\n",
    "testFile = dataDir+\"test-combined-0.0.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define function to parse our training and testing sets.\n",
    "\n",
    "**tokenizePhrase** takes a text message, removes @ mentions and # hashtags  as well as non-alphabetic characters and stop-words and returns the remaining words.\n",
    "\n",
    "**parseSentence** Takes a line from the training or testing file,  separates and parses the sentiment fields and the message field and returns an array of boolean sentiment flags as well as a tokenised version of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "KnownSentiments = [\"Anger\", \"Anticipation\", \"Disgust\", \"Fear\", \"Joy\", \"Sadness\", \"Surprise\", \"Trust\"]\n",
    "dropAts=True\n",
    "dropHashtags=True\n",
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def tokenizePhrase(phrase):\n",
    "    # Drop @-references used in social media texts.\n",
    "    if dropAts:\n",
    "        phrase = re.sub(\"@[^ ]*\", \"\", phrase)\n",
    "        \n",
    "    # Drop hashtags\n",
    "    if dropHashtags:\n",
    "        phrase = re.sub(\"#[^ ]*\", \"\", phrase)\n",
    "        \n",
    "    # Change non-alphabetic characters to spaces\n",
    "    phrase = re.sub(\"[^A-Za-z]\", \" \", phrase).lower()\n",
    "    \n",
    "    # Tokenize phrase while removing stop words and dropping tokens that are not more than 1 char long.\n",
    "    tokens = [w for w in word_tokenize(phrase) if w not in eng_stopwords and len(w)>1]\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "def parseSentence(sent):\n",
    "    # The sentiment labels are encoded at the beginning of the line as tab-separated fields\n",
    "    # Split the line by tabs so as to extract the labels and the text\n",
    "    parts = sent.split(\"\\t\")\n",
    "    \n",
    "    if len(parts)<9:\n",
    "        return ([], [])\n",
    "    \n",
    "    sentSents = parts[:8]\n",
    "    \n",
    "    # Match the sentiment labels with the known sentiments to extract a boolean vector \n",
    "    # encoding which sentiments are present.\n",
    "    sentMap = [sentSents[i]==KnownSentiments[i] for i in range(0, len(sentSents))]\n",
    "\n",
    "    \n",
    "    # The actual text\n",
    "    phrase = parts[8]\n",
    "    \n",
    "    tokens = tokenizePhrase(phrase)\n",
    "    \n",
    "    #return the sentiment map and the tokens extracted from the phrase\n",
    "    return (sentMap, tokens)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **BoW** class implements Bag of Words. \n",
    "\n",
    "The **fit** method takes a collection of tokens and adds them to the Bag of Words.\n",
    "\n",
    "The **transform** method takes a collection of tokens and returns a term-document vector correspondnding to the tokens\n",
    "\n",
    "The **fit_transform** method combines the other fit and transform methods into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "class BoW:\n",
    "\n",
    "    def fit(self, phraseTokens):\n",
    "        for tok in phraseTokens:\n",
    "            if tok not in self.vocabulary_:\n",
    "                tok_ndx = len(self.vocabulary_)\n",
    "                self.vocabulary_[tok]=tok_ndx\n",
    "                self.index_[tok_ndx]=tok\n",
    "                 \n",
    "    def transform(self, phraseTokens):\n",
    "        return [i for i in [self.vocabulary_.get(t, None) for t in phraseTokens] if i is not None]\n",
    "    \n",
    "    def fit_transform(self, phraseTokens):\n",
    "        self.fit(phraseTokens)\n",
    "        return self.transform(phraseTokens)\n",
    "    \n",
    "    vocabulary_=dict()\n",
    "    index_=dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class **NaiveBayes1** is an implementation of the Naive Bayes multinomial classifier algorithm. \n",
    "\n",
    "Apart from label-wise prediction, NaiveBayes1 also maintains pairwise probabilities of labels, so for every pair of labels (X,Y), the class keeps track of P(X|Y), P(¬X|Y), P(X|¬Y) and P(¬X|¬Y). It also has the capability of accepting a set of pairwise dependencies it can use to alter the final prediction probabilities. We can therefore, for example, set the classifier to consider the joint probability P(Anger, Fear) in order to attempt to take advantage of correlations between Anger and Fear to improve the prediction scores of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class NaiveBayes1:\n",
    "    def __init__(self, classes):\n",
    "\n",
    "        self.classes_=list(classes)\n",
    "        print \"Initialising NaiveBayes1 class with \", len(self.classes_), \" classes...\"\n",
    "        \n",
    "        self.globalCounts = np.zeros(1000) # counts for each word token\n",
    "        self.labelWordCounts = np.zeros((len(classes), 1000)) # counts for each word token per label\n",
    "        \n",
    "        # pairWiseCountsMatched is a matrix of pairwise label co-incidence counts\n",
    "        # i.e. for every instance where label x and label y are both true, element [x,y] is incremented\n",
    "        # Also for every instance where label x and label y are both false, element [y,x] is incremented\n",
    "        # Hence one half of the matrix holds positive incidence of labels while the other half hold negative incidence.\n",
    "        self.pairwiseCountsMatched = np.zeros((len(classes), len(classes))) \n",
    "        \n",
    "        # pairwiseCountsMismatched is a matrix of pairwise label non-incidence counts\n",
    "        # i.e. for every instance where label x is true and label y in false, element [x,y] is incremented\n",
    "        # for every instance where label x is false and label y is true, element [y,x] is incremented\n",
    "        self.pairwiseCountsMismatched = np.zeros((len(classes), len(classes)))\n",
    "        \n",
    "        # Holds pairwise dependencies between classes.\n",
    "        # A True in element [x,y] instructs the classifier to compute P(x,y:X) during prediction.\n",
    "        # This is used to hopefully leverage correlations between classes to improve predictions.\n",
    "        self.pairwiseIndex = np.zeros((len(classes), len(classes)))\n",
    "        \n",
    "        # labelCounts holds the incidence count for every class.\n",
    "        # i.e. for every training instance labelled with x, labelCounts[x] is incremented\n",
    "        self.labelCounts = np.zeros(len(classes))\n",
    "        \n",
    "        # The number of training instances\n",
    "        self.documentCount = 0\n",
    "        \n",
    "    # Update counts with a new training example\n",
    "    def update(self, labels, tokens):\n",
    "        self.documentCount += 1\n",
    "        \n",
    "        for x in range(0, len(self.classes_)):\n",
    "            #print(x)\n",
    "            if labels[x]:\n",
    "                self.labelCounts[x] += 1\n",
    "                \n",
    "            for y in range(0, len(self.classes_)):\n",
    "                if y>x:\n",
    "                    if labels[x]:\n",
    "                        if labels[y]:\n",
    "                            self.pairwiseCountsMatched[x, y] += 1\n",
    "                        else:\n",
    "                            self.pairwiseCountsMismatched[x, y] += 1\n",
    "                else:\n",
    "                    if not labels[x]:\n",
    "                        if not labels[y]:\n",
    "                            self.pairwiseCountsMatched[x, y] += 1\n",
    "                        else:\n",
    "                            self.pairwiseCountsMismatched[x, y] += 1\n",
    "                        \n",
    "            \n",
    "        for w in tokens:\n",
    "            toAdd=0\n",
    "            # Does the word exist in our vocab?\n",
    "            if w >= len(self.globalCounts):\n",
    "                # No - extend the counts array to accomodate\n",
    "                toAdd = int(math.ceil((w-len(self.globalCounts)+1) / 1000.0) * 1000)\n",
    "                self.globalCounts = np.append(self.globalCounts, np.zeros(toAdd))\n",
    "\n",
    "                \n",
    "            self.globalCounts[w] += 1\n",
    "\n",
    "            if toAdd>0:\n",
    "                self.labelWordCounts = np.append(self.labelWordCounts, np.zeros((len(self.classes_),toAdd)), axis=1)\n",
    "\n",
    "            for x in range(0, len(self.classes_)):\n",
    "                if labels[x]:\n",
    "                    self.labelWordCounts[x, w] += 1                            \n",
    "\n",
    "    # Recalculate all probabilities with the current counts\n",
    "    def recalc(self):\n",
    "        #print(\"self.documentCount\", self.documentCount)\n",
    "        #print(\"self.labelCounts=\", self.labelCounts)\n",
    "        globalTotal = sum(self.globalCounts)\n",
    "        #print(globalTotal)\n",
    "        \n",
    "        classProbs = np.array(self.labelWordCounts)\n",
    "        #print(\"classProbs\", classProbs.shape)\n",
    "        #print(\"sum(classProbs)\", sum(sum(classProbs)))\n",
    "\n",
    "        #print(\"self.labelWordCounts=\", self.labelWordCounts+1)\n",
    "        #print(\"self.globalCounts=\", self.globalCounts+len(classProbs)+1)\n",
    "        \n",
    "        # Laplace smoothed word probability by emotion - p(w|e) \n",
    "        # P(w|e) = count(w when e)/count(w)\n",
    "        self.wordClassPosProbs = (1.0 * classProbs+1)/(self.globalCounts+len(classProbs))# p(x|y)\n",
    "\n",
    "        #print(\"self.wordClassPosProbs\", self.wordClassPosProbs.shape)\n",
    "        #print(\"self.wordClassPosProbs=\", self.wordClassPosProbs)\n",
    "        \n",
    "        # emotion probability - p(e) \n",
    "        # p(e) = count()\n",
    "        # sum classProbs per ROW to find total counts per class\n",
    "        \n",
    "        self.posClassProbs = (1.0 * self.labelCounts) / self.documentCount\n",
    "        #print(\"self.posClassProbs\", self.posClassProbs)\n",
    "        #print(\"self.posClassProbs\", self.posClassProbs.shape)\n",
    "        \n",
    "        classProbs = self.globalCounts - self.labelWordCounts#np.array(self.globalCounts) - classProbs\n",
    "        #print(\"classProbs=\", classProbs+1)\n",
    "        \n",
    "        # Laplace smoothed word probability by negative emotion - p(w|¬e)\n",
    "        self.wordClassNegProbs = (1.0 * classProbs+1)/(self.globalCounts+len(classProbs)+1)\n",
    "\n",
    "        #print(\"self.wordClassNegProbs\", self.wordClassNegProbs.shape)\n",
    "        #print(\"self.wordClassNegProbs=\", self.wordClassNegProbs)\n",
    "        \n",
    "        # negative emotion probability - p(¬e) - element per class\n",
    "        self.negClassProbs = (1.0 *(self.documentCount-self.labelCounts)) / self.documentCount\n",
    "        #print(\"self.negClassProbs\", self.negClassProbs)\n",
    "        #print(\"self.negClassProbs\", self.negClassProbs.shape)\n",
    "        \n",
    "        #print(self.posClassProbs + self.negClassProbs)\n",
    "        \n",
    "        # pairwise emotion conditional probability - p(e1 | e2) - class x class\n",
    "        self.pairwiseClassProbs_x_y_matched = np.zeros((len(self.classes_), len(self.classes_)))\n",
    "        for x in range(0, len(self.classes_)):\n",
    "            for y in range(0, len(self.classes_)):\n",
    "                if x==y:\n",
    "                    self.pairwiseClassProbs_x_y_matched[x, y]=1\n",
    "                else:\n",
    "                    if y > x: # p(x|y)\n",
    "                        self.pairwiseClassProbs_x_y_matched[x, y] = (1.0*self.pairwiseCountsMatched[x, y])/(sum(self.labelWordCounts[y]))\n",
    "                        self.pairwiseClassProbs_x_y_matched[y, x] = (1.0*self.pairwiseCountsMatched[y, x])/(sum(self.globalCounts)-sum(self.labelWordCounts[y]))\n",
    "                     \n",
    "        # p(¬e1|e1), p(e1|¬e2)                \n",
    "        self.pairwiseClassProbs_x_y_mismatched = np.zeros((len(self.classes_), len(self.classes_)))\n",
    "        for x in range(0, len(self.classes_)):\n",
    "            for y in range(0, len(self.classes_)):\n",
    "                if x==y:\n",
    "                    self.pairwiseClassProbs_x_y_mismatched[x, y]=0\n",
    "                else:\n",
    "                    if y > x:  \n",
    "                        self.pairwiseClassProbs_x_y_mismatched[x, y] = (1.0*self.pairwiseCountsMismatched[x, y])/(sum(self.labelWordCounts[y])) # p(x|¬y)\n",
    "                        self.pairwiseClassProbs_x_y_mismatched[y, x] = (1.0*self.pairwiseCountsMismatched[y, x])/(sum(self.globalCounts)-sum(self.labelWordCounts[y])) # p(¬x|y)\n",
    "\n",
    "        # pairwise emotion conditional probability - p(e2 | e1) - class x class\n",
    "        self.pairwiseClassProbs_y_x_matched = np.zeros((len(self.classes_), len(self.classes_)))\n",
    "        for x in range(0, len(self.classes_)):\n",
    "            for y in range(0, len(self.classes_)):\n",
    "                if x==y:\n",
    "                    self.pairwiseClassProbs_y_x_matched[x, y]=1\n",
    "                else:\n",
    "                    if y > x: # p(y|x)\n",
    "                        self.pairwiseClassProbs_y_x_matched[x, y] = (1.0*self.pairwiseCountsMatched[x, y])/(sum(self.labelWordCounts[x]))\n",
    "                        self.pairwiseClassProbs_y_x_matched[y, x] = (1.0*self.pairwiseCountsMatched[y, x])/(sum(self.globalCounts)-sum(self.labelWordCounts[x])) #p(¬y|¬x)\n",
    "        \n",
    "        # p(¬e2|e1), p(e2|¬e1)               \n",
    "        self.pairwiseClassProbs_y_x_mismatched = np.zeros((len(self.classes_), len(self.classes_)))\n",
    "        for x in range(0, len(self.classes_)):\n",
    "            for y in range(0, len(self.classes_)):\n",
    "                if x==y:\n",
    "                    self.pairwiseClassProbs_y_x_mismatched[x, y]=0\n",
    "                else:\n",
    "                    if y > x: # p(y|¬x)\n",
    "                        self.pairwiseClassProbs_y_x_mismatched[x, y] = (1.0*self.pairwiseCountsMismatched[x, y])/(sum(self.labelWordCounts[x]))\n",
    "                        self.pairwiseClassProbs_y_x_mismatched[y, x] = (1.0*self.pairwiseCountsMismatched[y, x])/(sum(self.globalCounts)-sum(self.labelWordCounts[x])) #\\p(¬y|x)\n",
    "                        \n",
    "    # set pairs of classes to correlate in results\n",
    "    # classPairs : list(tuple(_0, _1))\n",
    "    #\n",
    "    # self.pairwiseIndex = matrix(len(classes), len(classes))\n",
    "    def setPariwiseDependencies(self, classPairs):\n",
    "        self.pairwiseIndex = np.zeros((len(self.classes_), len(self.classes_)))\n",
    "        for pair in classPairs:\n",
    "            self.pairwiseIndex[pair[0], pair[1]] = True\n",
    "      \n",
    "    # Return the trained conditional probability of classes X and Y\n",
    "    # ------+--------------           \n",
    "    #       \\   x_pos\n",
    "    # y_pos | true | false  \n",
    "    # ------+------+-------\n",
    "    # true  | x|y  | ¬x|y \n",
    "    # false | x|¬y | ¬x|¬y\n",
    "    #-------+------+------\n",
    "    def getConditionalProbability(self, X, Y, x_pos, y_pos):\n",
    "        if Y > X:\n",
    "            if x_pos:\n",
    "                if not y_pos:\n",
    "                    #print \"#b\", key, value\n",
    "                    return self.pairwiseClassProbs_x_y_mismatched[X, Y]\n",
    "                else:\n",
    "                    return self.pairwiseClassProbs_x_y_matched[X, Y]\n",
    "\n",
    "            else:\n",
    "                if not y_pos:\n",
    "                    return self.pairwiseClassProbs_x_y_matched[Y, X]\n",
    "                else:\n",
    "                    return self.pairwiseClassProbs_x_y_mismatched[Y, X]\n",
    "        else:\n",
    "            if x_pos:\n",
    "                if not y_pos:\n",
    "                    return self.pairwiseClassProbs_y_x_mismatched[Y, X]\n",
    "                else:\n",
    "\n",
    "                    return self.pairwiseClassProbs_y_x_matched[Y, X]\n",
    "            else:\n",
    "                if not y_pos:\n",
    "                    return self.pairwiseClassProbs_y_x_matched[ X, Y] \n",
    "                else:\n",
    "                    return self.pairwiseClassProbs_y_x_mismatched[X, Y] \n",
    "\n",
    "    \n",
    "    # Classify a tokenised test instance\n",
    "    def classify(self, tokens):\n",
    "        # Predictions per class\n",
    "        classPredictions = [False] * len(self.classes_)\n",
    "        \n",
    "        # class probabilities; positive and negative\n",
    "        posClassProbs=[1.0]*len(classPredictions)\n",
    "        negClassProbs=[1.0]*len(classPredictions)\n",
    "        \n",
    "        # Compute P(Class|Token) and P(¬Class|Token)\n",
    "        for tok in tokens:\n",
    "            if tok < len(self.globalCounts):\n",
    "                posClassProbs *= self.wordClassPosProbs[:, tok]\n",
    "                negClassProbs *= self.wordClassNegProbs[:, tok]\n",
    "                \n",
    "        posClassProbs *= self.posClassProbs\n",
    "        negClassProbs *= self.negClassProbs\n",
    "        \n",
    "        preds = posClassProbs>negClassProbs\n",
    "\n",
    "        #p(X, Y | W) = p(X|Y).p(X).p(Y).Prod(w|X, w|Y)\n",
    "        for x in range(0, len(self.classes_)):\n",
    "            for y in range(0, len(self.classes_)):\n",
    "                if self.pairwiseIndex[x, y]:\n",
    "                    probs = np.zeros((2,2))\n",
    "\n",
    "                    probs[1, 1] = self.getConditionalProbability(x, y, True, True)\n",
    "                    probs[1, 0] = self.getConditionalProbability(x, y, True, False)\n",
    "                    probs[0, 1] = self.getConditionalProbability(x, y, False, True)\n",
    "                    probs[0, 0] = self.getConditionalProbability(x, y, False, False)\n",
    "\n",
    "                    probs[1,:] *= posClassProbs[x]\n",
    "                    probs[0,:] *= negClassProbs[x]\n",
    "                    probs[:,1] *= posClassProbs[y]\n",
    "                    probs[:,0] *= negClassProbs[y]\n",
    "\n",
    "                    argmax = np.argmax(probs)\n",
    "                    argmax_prob = (argmax/probs.shape[1], argmax % probs.shape[1])\n",
    "\n",
    "                    preds[x] = bool(argmax_prob[0])\n",
    "                    preds[y] = bool(argmax_prob[1])\n",
    "\n",
    "        \n",
    "        return preds\n",
    "                    \n",
    "        \n",
    "    classes_=list()\n",
    "    globalCounts=np.array([]) # array of counts indexed by token id\n",
    "    labelWordCounts=np.array([]) # array of classes_ rows x tokens columns\n",
    "    # array classes x classes x tokens\n",
    "    # for y>x (top diagonal half) contains counts where sentiment(x) and sentiment(y)\n",
    "    # for x<y (bottom diagonal half) contains counts where not(sentiment(x)) and not(sentiment(y))\n",
    "    pairwiseCountsMatched=np.array([]) \n",
    "    \n",
    "    # array classes x classes x tokens\n",
    "    # for y>x (top diagonal half) contains counts where sentiment(x) and not(sentiment(y))\n",
    "    # for x<y (bottom diagonal half) contains counts where not(sentiment(x)) and sentiment(y)\n",
    "    pairwiseCountsMismatched=np.array([]) \n",
    "    classPairs=dict() # dict of emotion id -> emotion id\n",
    "    pairwiseIndex=np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class ModelScores calculates and maintains prediction scores for a classificiation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelScores:\n",
    "    def __init__(self, knownClasses):\n",
    "        self.knownClasses = knownClasses\n",
    "        self.truePositives = np.zeros(len(knownClasses))\n",
    "        self.trueNegatives = np.zeros(len(knownClasses))\n",
    "        self.falsePositives = np.zeros(len(knownClasses))\n",
    "        self.falseNegatives = np.zeros(len(knownClasses))\n",
    "        \n",
    "    def accumulate(self, predictedLabels, trainingLabels):\n",
    "        self.truePositives += np.logical_and(predictedLabels, trainingLabels)\n",
    "        self.trueNegatives += np.logical_and(np.logical_not(predictedLabels), np.logical_not(trainingLabels))\n",
    "        self.falsePositives += np.logical_and(preds, np.logical_not(trainingLabels))\n",
    "        self.falseNegatives += np.logical_and(np.logical_not(preds), trainingLabels)\n",
    "\n",
    "    def getStats(self):\n",
    "        precision = self.truePositives / (self.truePositives+self.falsePositives)\n",
    "        recall = self.truePositives / (self.truePositives+self.falseNegatives)\n",
    "        accuracy = (self.truePositives+self.trueNegatives)/(self.truePositives+self.trueNegatives+self.falsePositives+self.falseNegatives)\n",
    "        \n",
    "        f1 = 2* ((precision * recall)/(precision + recall))\n",
    "        \n",
    "        return (accuracy, precision, recall, f1)\n",
    "    \n",
    "    def printScores(self):\n",
    "        (accuracy, precision, recall, f1score) = self.getStats()\n",
    "        print self.knownClasses\n",
    "        print \"true positives = \", self.truePositives\n",
    "        print \"true negatives = \", self.trueNegatives\n",
    "        print \"false positives = \", self.falsePositives\n",
    "        print \"false negatives = \", self.falseNegatives\n",
    "        print\n",
    "        print \"accuracy=\", np.round(accuracy, 3)\n",
    "        print \"precision=\", np.round(precision, 3)\n",
    "        print \"recall=\", np.round(recall, 3)\n",
    "        print \"f1score=\", np.round(f1score, 3)\n",
    "        \n",
    "    def getDeltas(self, other):\n",
    "        (o_accuracy, o_precision, o_recall, o_f1) = other.getStats()\n",
    "        (accuracy, precision, recall, f1) = self.getStats()\n",
    "        \n",
    "        return (accuracy-o_accuracy, precision-o_precision, recall-o_recall, f1-o_f1)\n",
    "    \n",
    "    def printDeltas(self, other):\n",
    "        (accuracy, precision, recall, f1) = self.getDeltas(other)\n",
    "        \n",
    "        print \"delta accuracy=\", np.round(accuracy, 3)\n",
    "        print \"delta precision=\", np.round(precision, 3)\n",
    "        print \"delta recall=\", np.round(recall, 3)\n",
    "        print \"delta f1score=\", np.round(f1, 3)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Individual Emotions\n",
    "\n",
    "Predict individual emotions based on the training data. \n",
    "\n",
    "No dependencies between different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising NaiveBayes1 class with  8  classes...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "bow = BoW() # Create empty Bag of Words\n",
    "nb1 = NaiveBayes1(KnownSentiments) # Create Naive Bayes class with the known sentiments as labels.\n",
    "i=0\n",
    "# Process the training file line-by-line\n",
    "for line in open(trainFile):\n",
    "    #print i, line\n",
    "    (classMap, tokens)=parseSentence(line) # Parse each sentence returning sentiments and tokens\n",
    "    #print(i, \": \", classMap)\n",
    "    if len(classMap)==0:\n",
    "        i+=1\n",
    "        continue\n",
    "        \n",
    "    token_ndx = bow.fit_transform(tokens) # Transform tokens to term-document frequency\n",
    "    \n",
    "    nb1.update(classMap, token_ndx) # Update the Naive Bayes class with the new training data\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "nb1.recalc() # Recalculate the probabilities\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust']\n",
      "true positives =  [1176.  977.  477.  105.  353.  956.    2.  115.]\n",
      "true negatives =  [ 180.  250.  777. 1099.  938.  200. 1426. 1233.]\n",
      "false positives =  [531. 501. 267.  57. 261. 695.   3.  42.]\n",
      "false negatives =  [ 69. 228. 435. 695. 404. 105. 525. 566.]\n",
      "\n",
      "accuracy= [0.693 0.627 0.641 0.616 0.66  0.591 0.73  0.689]\n",
      "precision= [0.689 0.661 0.641 0.648 0.575 0.579 0.4   0.732]\n",
      "recall= [0.945 0.811 0.523 0.131 0.466 0.901 0.004 0.169]\n",
      "f1score= [0.797 0.728 0.576 0.218 0.515 0.705 0.008 0.274]\n"
     ]
    }
   ],
   "source": [
    "nb1.setPariwiseDependencies({}) # Assume NO pairwise dependencies between labels\n",
    "origScores = ModelScores(KnownSentiments)\n",
    "\n",
    "# Process the test file\n",
    "for line in open(testFile):\n",
    "    # Parse, tokenise and BoW...\n",
    "    (testClassMap, tokens) = parseSentence(line)\n",
    "    if (tokens is None):\n",
    "        continue\n",
    "    tokens = bow.transform(tokens)\n",
    "    # ... and use Naive Bayes to classify\n",
    "    preds = nb1.classify(tokens)\n",
    "     \n",
    "    origScores.accumulate(preds, testClassMap)\n",
    "\n",
    "origScores.printScores()\n",
    "(orig_accuracy, orig_precision, orig_recall, orig_f1) = origScores.getStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Manual addition of relationships between emotion labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Pariwise combinations of 2 emotion labels.\n",
    "\n",
    "We notice from the above scores that while Sadness has the worst accuracy, Surprise has by far the worst f1-score, due to a very low recall. In fact, our classifier seems very reluctant to classify Sadness, having only scored 2 true positives on this label, compared to 525 false negatives.\n",
    "\n",
    "We tried to improve the score on the Surprise label by adding a relationship to another emotion and we found that pairing Surprise to Anger gave the best, albeit modest, improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust']\n",
      "true positives =  [1176. 1011.  477.  105.  353.  956.    3.  115.]\n",
      "true negatives =  [ 180.  220.  777. 1099.  938.  200. 1427. 1233.]\n",
      "false positives =  [531. 531. 267.  57. 261. 695.   2.  42.]\n",
      "false negatives =  [ 69. 194. 435. 695. 404. 105. 524. 566.]\n",
      "\n",
      "accuracy= [0.693 0.629 0.641 0.616 0.66  0.591 0.731 0.689]\n",
      "precision= [0.689 0.656 0.641 0.648 0.575 0.579 0.6   0.732]\n",
      "recall= [0.945 0.839 0.523 0.131 0.466 0.901 0.006 0.169]\n",
      "f1score= [0.797 0.736 0.576 0.218 0.515 0.705 0.011 0.274]\n",
      "delta accuracy= [0.    0.002 0.    0.    0.    0.    0.001 0.   ]\n",
      "delta precision= [ 0.    -0.005  0.     0.     0.     0.     0.2    0.   ]\n",
      "delta recall= [0.    0.028 0.    0.    0.    0.    0.002 0.   ]\n",
      "delta f1score= [0.    0.008 0.    0.    0.    0.    0.004 0.   ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anger | Sadness\n",
    "nb1.setPariwiseDependencies([(6,1)])\n",
    "scores = ModelScores(KnownSentiments)\n",
    "\n",
    "for line in open(testFile):\n",
    "    (testClassMap, tokens) = parseSentence(line)\n",
    "    if (tokens is None):\n",
    "        continue\n",
    "    tokens = bow.transform(tokens)\n",
    "    preds = nb1.classify(tokens)\n",
    "\n",
    "    scores.accumulate(preds, testClassMap)\n",
    "\n",
    "scores.printScores()\n",
    "\n",
    "scores.printDeltas(origScores)\n",
    "\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Best set of pairwise variable assignments\n",
    "We now want to search through all pairwise combinations of emotions and all four combinations of conditional probability assignments (x|y, x|¬y, ¬x|y, ¬x|¬y) to find the best assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluating P( Anger , Anticipation  | X)\n",
      "(-0.001642360451014624, ':', -0.0016423604510146378)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47602870374123296 ( -0.001642360451014624 )\n",
      "Evaluating P( Anger , Disgust  | X)\n",
      "(0.006635081319519287, ':', 0.006635081319519287)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4843061455117669 ( 0.006635081319519287 )\n",
      "Evaluating P( Anger , Fear  | X)\n",
      "(-0.0050065677835374744, ':', -0.005006567783537461)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4726644964087101 ( -0.0050065677835374744 )\n",
      "Evaluating P( Anger , Joy  | X)\n",
      "(-0.007342081390642297, ':', -0.0073420813906423035)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4703289828016053 ( -0.007342081390642297 )\n",
      "Evaluating P( Anger , Sadness  | X)\n",
      "(0.0009198558938562362, ':', 0.0009198558938562224)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4785909200861038 ( 0.0009198558938562362 )\n",
      "Evaluating P( Anger , Surprise  | X)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etienne/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Anger , Trust  | X)\n",
      "(-0.005444459350626429, ':', -0.005444459350626412)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47222660484162116 ( -0.005444459350626429 )\n",
      "Evaluating P( Anticipation , Anger  | X)\n",
      "(-0.005093717398842568, ':', -0.005093717398842623)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.472577346793405 ( -0.005093717398842568 )\n",
      "Evaluating P( Anticipation , Disgust  | X)\n",
      "(-0.0057622878853967885, ':', -0.005762287885396858)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4719087763068508 ( -0.0057622878853967885 )\n",
      "Evaluating P( Anticipation , Fear  | X)\n",
      "(-0.0067718454252605476, ':', -0.006771845425260523)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47089921876698704 ( -0.0067718454252605476 )\n",
      "Evaluating P( Anticipation , Joy  | X)\n",
      "(0.0014774749586726754, ':', 0.0014774749586727032)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47914853915092026 ( 0.0014774749586726754 )\n",
      "Evaluating P( Anticipation , Sadness  | X)\n",
      "(-0.0016019098937857246, ':', -0.0016019098937857384)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47606915429846186 ( -0.0016019098937857246 )\n",
      "Evaluating P( Anticipation , Surprise  | X)\n",
      "(0.0046823278343668395, ':', 0.004682327834366804)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4823533920266144 ( 0.0046823278343668395 )\n",
      "Evaluating P( Anticipation , Trust  | X)\n",
      "(-0.0010874096842085623, ':', -0.0010874096842085484)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.476583654508039 ( -0.0010874096842085623 )\n",
      "Evaluating P( Disgust , Anger  | X)\n",
      "(0.003414307927609883, ':', 0.003414307927609911)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.48108537211985747 ( 0.003414307927609883 )\n",
      "Evaluating P( Disgust , Anticipation  | X)\n",
      "(-0.007552182572768129, ':', -0.007552182572768129)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47011888161947946 ( -0.007552182572768129 )\n",
      "Evaluating P( Disgust , Fear  | X)\n",
      "(-0.006411176284998876, ':', -0.006411176284998949)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4712598879072487 ( -0.006411176284998876 )\n",
      "Evaluating P( Disgust , Joy  | X)\n",
      "(0.004879523852727385, ':', 0.004879523852727385)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.48255058804497497 ( 0.004879523852727385 )\n",
      "Evaluating P( Disgust , Sadness  | X)\n",
      "(0.00375586807822359, ':', 0.00375586807822359)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4814269322704712 ( 0.00375586807822359 )\n",
      "Evaluating P( Disgust , Surprise  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Disgust , Trust  | X)\n",
      "(0.010669109815159339, ':', 0.010669109815159304)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4883401740074069 ( 0.010669109815159339 )\n",
      "Evaluating P( Fear , Anger  | X)\n",
      "(-0.012010116541837368, ':', -0.012010116541837354)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4656609476504102 ( -0.012010116541837368 )\n",
      "Evaluating P( Fear , Anticipation  | X)\n",
      "(-0.009045242807997234, ':', -0.00904524280799724)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.46862582138425035 ( -0.009045242807997234 )\n",
      "Evaluating P( Fear , Disgust  | X)\n",
      "(-0.013659787400198908, ':', -0.013659787400198978)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4640112767920487 ( -0.013659787400198908 )\n",
      "Evaluating P( Fear , Joy  | X)\n",
      "(-0.0010853923806166677, ':', -0.0010853923806167058)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4765856718116309 ( -0.0010853923806166677 )\n",
      "Evaluating P( Fear , Sadness  | X)\n",
      "(0.005806208443920657, ':', 0.005806208443920616)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.48347727263616824 ( 0.005806208443920657 )\n",
      "Evaluating P( Fear , Surprise  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Fear , Trust  | X)\n",
      "(-0.0011371795675763074, ':', -0.0011371795675763074)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4765338846246713 ( -0.0011371795675763074 )\n",
      "Evaluating P( Joy , Anger  | X)\n",
      "(-0.012662592565158559, ':', -0.012662592565158573)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.465008471627089 ( -0.012662592565158559 )\n",
      "Evaluating P( Joy , Anticipation  | X)\n",
      "(-0.0017452564916597413, ':', -0.001745256491659783)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47592580770058784 ( -0.0017452564916597413 )\n",
      "Evaluating P( Joy , Disgust  | X)\n",
      "(0.0043465603982680845, ':', 0.004346560398268029)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.48201762459051567 ( 0.0043465603982680845 )\n",
      "Evaluating P( Joy , Fear  | X)\n",
      "(3.636806252704794e-06, ':', 3.6368062526562217e-06)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4776747009985003 ( 3.636806252704794e-06 )\n",
      "Evaluating P( Joy , Sadness  | X)\n",
      "(-0.00685999939654669, ':', -0.006859999396546711)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4708110647957009 ( -0.00685999939654669 )\n",
      "Evaluating P( Joy , Surprise  | X)\n",
      "(0.0072825240220578125, ':', 0.007282524022057778)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4849535882143054 ( 0.0072825240220578125 )\n",
      "Evaluating P( Joy , Trust  | X)\n",
      "(-4.8637638699278174e-05, ':', -4.863763869929899e-05)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4776224265535483 ( -4.8637638699278174e-05 )\n",
      "Evaluating P( Sadness , Anger  | X)\n",
      "(0.0006183339132306287, ':', 0.0006183339132306287)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4782893981054782 ( 0.0006183339132306287 )\n",
      "Evaluating P( Sadness , Anticipation  | X)\n",
      "(-0.0002247792900340606, ':', -0.0002247792900340606)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4774462849022135 ( -0.0002247792900340606 )\n",
      "Evaluating P( Sadness , Disgust  | X)\n",
      "(0.006092191324433505, ':', 0.006092191324433421)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4837632555166811 ( 0.006092191324433505 )\n",
      "Evaluating P( Sadness , Fear  | X)\n",
      "(0.02070378128161554, ':', 0.02070378128161543)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4983748454738631 ( 0.02070378128161554 )\n",
      "Evaluating P( Sadness , Joy  | X)\n",
      "(-0.0032443815873469672, ':', -0.0032443815873470228)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4744266826049006 ( -0.0032443815873469672 )\n",
      "Evaluating P( Sadness , Surprise  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Sadness , Trust  | X)\n",
      "(-0.006699677028158102, ':', -0.0066996770281580985)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4709713871640895 ( -0.006699677028158102 )\n",
      "Evaluating P( Surprise , Anger  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Surprise , Anticipation  | X)\n",
      "(0.0014432361233498447, ':', 0.0014432361233498135)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47911430031559743 ( 0.0014432361233498447 )\n",
      "Evaluating P( Surprise , Disgust  | X)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etienne/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Surprise , Fear  | X)\n",
      "(-0.010113478863478798, ':', -0.010113478863478867)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4675575853287688 ( -0.010113478863478798 )\n",
      "Evaluating P( Surprise , Joy  | X)\n",
      "(-0.0010597838473834775, ':', -0.0010597838473834706)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4766112803448641 ( -0.0010597838473834775 )\n",
      "Evaluating P( Surprise , Sadness  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Surprise , Trust  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Trust , Anger  | X)\n",
      "(-0.004941079993604491, ':', -0.004941079993604481)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4727299841986431 ( -0.004941079993604491 )\n",
      "Evaluating P( Trust , Anticipation  | X)\n",
      "(-0.005751196035703676, ':', -0.005751196035703659)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4719198681565439 ( -0.005751196035703676 )\n",
      "Evaluating P( Trust , Disgust  | X)\n",
      "(0.009642641338028668, ':', 0.009642641338028633)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.48731370553027625 ( 0.009642641338028668 )\n",
      "Evaluating P( Trust , Fear  | X)\n",
      "(-0.004685918133076505, ':', -0.004685918133076537)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4729851460591711 ( -0.004685918133076505 )\n",
      "Evaluating P( Trust , Joy  | X)\n",
      "(-0.006215534385696053, ':', -0.006215534385696102)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47145552980655153 ( -0.006215534385696053 )\n",
      "Evaluating P( Trust , Sadness  | X)\n",
      "(-0.007586595803451557, ':', -0.007586595803451581)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47008446838879603 ( -0.007586595803451557 )\n",
      "Evaluating P( Trust , Surprise  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "(origAccuracy, origPrecision, origRecall, origf1) = origScores.getStats()\n",
    "mean_orig_f1 = np.mean(origf1)\n",
    "pairwiseImprovements = np.zeros((len(KnownSentiments), len(KnownSentiments)))\n",
    "\n",
    "for x in range(0, len(KnownSentiments)):\n",
    "    for y in range(0, len(KnownSentiments)):\n",
    "        if x!=y:\n",
    "            sentx = KnownSentiments[x]\n",
    "            senty = KnownSentiments[y]                \n",
    "                \n",
    "            print \"Evaluating P(\", sentx, \",\", senty, \" | X)\"\n",
    "                   \n",
    "            nb1.setPariwiseDependencies([(x, y)])\n",
    "\n",
    "            scores = ModelScores(KnownSentiments)\n",
    "\n",
    "            for line in open(testFile):\n",
    "                (testClassMap, tokens) = parseSentence(line)\n",
    "                if (tokens is None):\n",
    "                    continue\n",
    "                tokens = bow.transform(tokens)\n",
    "                preds = nb1.classify(tokens)\n",
    "\n",
    "                scores.accumulate(preds, testClassMap)\n",
    "\n",
    "            (a, p, r, f1) = scores.getStats()\n",
    "            (d_a, d_p, d_r, d_f1) = scores.getDeltas(origScores)\n",
    "            \n",
    "            mean_f1 = np.mean(f1)\n",
    "            mean_delta_f1 = mean_f1 - mean_orig_f1\n",
    "            print(mean_delta_f1, \":\", np.mean(d_f1))\n",
    "            #deltas[combination, y] = d_a[x]\n",
    "            print \"    Edge changed mean f1-score from \", mean_orig_f1, \" to \", mean_f1, \"(\" , (mean_f1-mean_orig_f1), \")\"\n",
    "            pairwiseImprovements[x, y] = mean_delta_f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beneficial Edge:  Sadness | Fear : f1-score + 0.02070378128161554\n",
      "(5, 3)\n",
      "Beneficial Edge:  Disgust | Trust : f1-score + 0.010669109815159339\n",
      "(2, 7)\n",
      "Beneficial Edge:  Trust | Disgust : f1-score + 0.009642641338028668\n",
      "Beneficial Edge:  Joy | Surprise : f1-score + 0.0072825240220578125\n",
      "(4, 6)\n",
      "Beneficial Edge:  Anger | Disgust : f1-score + 0.006635081319519287\n",
      "Beneficial Edge:  Sadness | Disgust : f1-score + 0.006092191324433505\n",
      "Beneficial Edge:  Fear | Sadness : f1-score + 0.005806208443920657\n",
      "Beneficial Edge:  Disgust | Joy : f1-score + 0.004879523852727385\n",
      "Beneficial Edge:  Anticipation | Surprise : f1-score + 0.0046823278343668395\n",
      "Beneficial Edge:  Joy | Disgust : f1-score + 0.0043465603982680845\n",
      "Beneficial Edge:  Disgust | Sadness : f1-score + 0.00375586807822359\n",
      "Beneficial Edge:  Disgust | Anger : f1-score + 0.003414307927609883\n",
      "Beneficial Edge:  Anticipation | Joy : f1-score + 0.0014774749586726754\n",
      "Beneficial Edge:  Surprise | Anticipation : f1-score + 0.0014432361233498447\n",
      "Beneficial Edge:  Anger | Sadness : f1-score + 0.0009198558938562362\n",
      "Beneficial Edge:  Sadness | Anger : f1-score + 0.0006183339132306287\n",
      "Beneficial Edge:  Joy | Fear : f1-score + 3.636806252704794e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    print \"Beneficial edges for \", KnownSentiments[x], \":\"\\n    for y in range(0, len(deltas)):\\n        for combination in range(0, 4, 2):\\n            if x!=y:\\n                # Avoid adding both x|y and x|\\xc2\\xacy - Choose the best option instead\\n                if (deltas[combination, y]>0) or (deltas[combination+1, y]>0): \\n                    if deltas[combination, y] > deltas[combination+1, y]:\\n                        bestCombination = combination\\n                    else:\\n                        bestCombination = combination+1\\n                        \\n                    x1=x+1\\n                    y1=y+1\\n                    sentx = KnownSentiments[x]\\n                    senty = KnownSentiments[y]\\n\\n                    if bestCombination & 1:\\n                        y1=-y1\\n                        senty=\"!\"+senty\\n\\n                    if bestCombination & 2:\\n                        x1=-x1\\n                        sentx=\"!\"+sentx\\n\\n                    print \"P(\", sentx, \"|\", senty, \"), delta=\", deltas[bestCombination, y]\\n                    if x1 in globalPariwiseDependencies:\\n                        globalPariwiseDependencies[x1].append(y1)\\n                    else:\\n                        globalPariwiseDependencies[x1]=[y1]\\n'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the assignments that gave the best improvements\n",
    "sortedNdxs = np.argsort(pairwiseImprovements.flatten())[::-1]\n",
    "usedLabels = set()\n",
    "globalPariwiseAssignments = list()\n",
    "\n",
    "for ndx in sortedNdxs:\n",
    "    #print ndx\n",
    "    x = ndx / len(KnownSentiments)\n",
    "    y = ndx % len(KnownSentiments)\n",
    "   # print(x, y)\n",
    "    if pairwiseImprovements[x, y]>0:\n",
    "        print \"Beneficial Edge: \", KnownSentiments[x], \"|\", KnownSentiments[y], \": f1-score +\", pairwiseImprovements[x, y]\n",
    "        \n",
    "        if (x not in usedLabels) and (y not in usedLabels):\n",
    "            usedLabels.update([x])\n",
    "            usedLabels.update([y])\n",
    "            print(x, y)\n",
    "            globalPariwiseAssignments.extend([(x, y)])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now re-classify the test set using the discovers relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 3), (2, 7), (4, 6)]\n",
      "['Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust']\n",
      "true positives =  [1.176e+03 9.770e+02 6.320e+02 2.550e+02 5.380e+02 7.980e+02 1.000e+00\n",
      " 1.300e+02]\n",
      "true negatives =  [ 180.  250.  584.  976.  629.  393. 1429. 1234.]\n",
      "false positives =  [531. 501. 460. 180. 570. 502.   0.  41.]\n",
      "false negatives =  [ 69. 228. 280. 545. 219. 263. 526. 551.]\n",
      "\n",
      "accuracy= [0.693 0.627 0.622 0.629 0.597 0.609 0.731 0.697]\n",
      "precision= [0.689 0.661 0.579 0.586 0.486 0.614 1.    0.76 ]\n",
      "recall= [0.945 0.811 0.693 0.319 0.711 0.752 0.002 0.191]\n",
      "f1score= [0.797 0.728 0.631 0.413 0.577 0.676 0.004 0.305]\n",
      "delta accuracy= [ 0.     0.    -0.019  0.014 -0.063  0.018  0.001  0.008]\n",
      "delta precision= [ 0.     0.    -0.062 -0.062 -0.089  0.035  0.6    0.028]\n",
      "delta recall= [ 0.     0.     0.17   0.187  0.244 -0.149 -0.002  0.022]\n",
      "delta f1score= [ 0.     0.     0.055  0.195  0.062 -0.029 -0.004  0.031]\n",
      "\n",
      "Change in f1-score:  0.4776710641922476 -> 0.5163264793110801  =  0.038655415118832526\n"
     ]
    }
   ],
   "source": [
    "print globalPariwiseAssignments\n",
    "\n",
    "nb1.setPariwiseDependencies(globalPariwiseAssignments)\n",
    "\n",
    "scores = ModelScores(KnownSentiments)\n",
    "\n",
    "for line in open(testFile):\n",
    "    (testClassMap, tokens) = parseSentence(line)\n",
    "    if (tokens is None):\n",
    "        continue\n",
    "    tokens = bow.transform(tokens)\n",
    "    preds = nb1.classify(tokens)\n",
    "     \n",
    "    scores.accumulate(preds, testClassMap)\n",
    "\n",
    "scores.printScores()\n",
    "\n",
    "scores.printDeltas(origScores)\n",
    "\n",
    "(a, p, r, f1) = scores.getStats()\n",
    "\n",
    "print\n",
    "print \"Change in f1-score: \", mean_orig_f1, \"->\", np.mean(f1), \" = \", (np.mean(f1)-mean_orig_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above results it appears that we managed to increase the scores in all classes except anticipation which dropped .5% in accuracy. Clearly this strategy, while, on the whole, increasing the predictive performance of th model is not optimal. A perfect method would be to go through th entire search space of the problem, but for pairwise matching of emotions that would mean (8x8x4)! possible configurations, but this is not tractable.\n",
    "\n",
    "A possible better method to search through this state space would possibly be a genetic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 0.]\n",
      " [2. 0. 5.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "t = np.zeros((2, 3))\n",
    "t[0,0]=1\n",
    "t[1,0]=2\n",
    "t[0,1]=2\n",
    "t[1,2]=5\n",
    "print(t)\n",
    "\n",
    "#print(len(t))\n",
    "#t2 = np.sum(t, axis=1)\n",
    "#print(t2)\n",
    "#t2/sum(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 2., 5.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 1.66666667]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = [2, 2, 3]\n",
    "\n",
    "t/t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 5.],\n",
       "       [1., 2., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t)-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., 10.,  0.],\n",
       "       [ 2.,  0.,  5.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t\n",
    "t[0,1] = 10\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(t)/t.shape[1], np.argmax(t) % t.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
