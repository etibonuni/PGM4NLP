{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection in Twitter Data\n",
    "The scope of this notebook is to implement a Naive-Bayes classifier that can be used to predict emotion in Twitter messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the location of the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "dataDir = \"Data/ssec-aggregated/\"\n",
    "trainFile = dataDir+\"train-combined-0.0.csv\"\n",
    "testFile = dataDir+\"test-combined-0.0.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define function to parse our training and testing sets.\n",
    "\n",
    "**tokenizePhrase** takes a text message, removes @ mentions and # hashtags  as well as non-alphabetic characters and stop-words and returns the remaining words.\n",
    "\n",
    "**parseSentence** Takes a line from the training or testing file,  separates and parses the sentiment fields and the message field and returns an array of boolean sentiment flags as well as a tokenised version of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "KnownSentiments = [\"Anger\", \"Anticipation\", \"Disgust\", \"Fear\", \"Joy\", \"Sadness\", \"Surprise\", \"Trust\"]\n",
    "dropAts=True\n",
    "dropHashtags=True\n",
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def tokenizePhrase(phrase):\n",
    "    # Drop @-references used in social media texts.\n",
    "    if dropAts:\n",
    "        phrase = re.sub(\"@[^ ]*\", \"\", phrase)\n",
    "        \n",
    "    # Drop hashtags\n",
    "    if dropHashtags:\n",
    "        phrase = re.sub(\"#[^ ]*\", \"\", phrase)\n",
    "        \n",
    "    # Change non-alphabetic characters to spaces\n",
    "    phrase = re.sub(\"[^A-Za-z]\", \" \", phrase).lower()\n",
    "    \n",
    "    # Tokenize phrase while removing stop words and dropping tokens that are not more than 1 char long.\n",
    "    tokens = [w for w in word_tokenize(phrase) if w not in eng_stopwords and len(w)>1]\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "def parseSentence(sent):\n",
    "    # The sentiment labels are encoded at the beginning of the line as tab-separated fields\n",
    "    # Split the line by tabs so as to extract the labels and the text\n",
    "    parts = sent.split(\"\\t\")\n",
    "    \n",
    "    if len(parts)<9:\n",
    "        return ([], [])\n",
    "    \n",
    "    sentSents = parts[:8]\n",
    "    \n",
    "    # Match the sentiment labels with the known sentiments to extract a boolean vector \n",
    "    # encoding which sentiments are present.\n",
    "    sentMap = [sentSents[i]==KnownSentiments[i] for i in range(0, len(sentSents))]\n",
    "\n",
    "    \n",
    "    # The actual text\n",
    "    phrase = parts[8]\n",
    "    \n",
    "    tokens = tokenizePhrase(phrase)\n",
    "    \n",
    "    #return the sentiment map and the tokens extracted from the phrase\n",
    "    return (sentMap, tokens)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **BoW** class implements Bag of Words. \n",
    "\n",
    "The **fit** method takes a collection of tokens and adds them to the Bag of Words.\n",
    "\n",
    "The **transform** method takes a collection of tokens and returns a term-document vector correspondnding to the tokens\n",
    "\n",
    "The **fit_transform** method combines the other fit and transform methods into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "class BoW:\n",
    "\n",
    "    def fit(self, phraseTokens):\n",
    "        for tok in phraseTokens:\n",
    "            if tok not in self.vocabulary_:\n",
    "                tok_ndx = len(self.vocabulary_)\n",
    "                self.vocabulary_[tok]=tok_ndx\n",
    "                self.index_[tok_ndx]=tok\n",
    "                 \n",
    "    def transform(self, phraseTokens):\n",
    "        return [i for i in [self.vocabulary_.get(t, None) for t in phraseTokens] if i is not None]\n",
    "    \n",
    "    def fit_transform(self, phraseTokens):\n",
    "        self.fit(phraseTokens)\n",
    "        return self.transform(phraseTokens)\n",
    "    \n",
    "    vocabulary_=dict()\n",
    "    index_=dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class **NaiveBayes1** is an implementation of the Naive Bayes multinomial classifier algorithm. \n",
    "\n",
    "Apart from label-wise prediction, NaiveBayes1 also maintains pairwise probabilities of labels, so for every pair of labels (X,Y), the class keeps track of P(X|Y), P(¬X|Y), P(X|¬Y) and P(¬X|¬Y). It also has the capability of accepting a set of pairwise dependencies it can use to alter the final prediction probabilities. We can therefore, for example, set the classifier to consider the joint probability P(Anger, Fear) in order to attempt to take advantage of correlations between Anger and Fear to improve the prediction scores of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class NaiveBayes1:\n",
    "    def __init__(self, classes):\n",
    "\n",
    "        self.classes_=list(classes)\n",
    "        print \"Initialising NaiveBayes1 class with \", len(self.classes_), \" classes...\"\n",
    "        \n",
    "        self.globalCounts = np.zeros(1000) # counts for each word token\n",
    "        self.labelWordCounts = np.zeros((len(classes), 1000)) # counts for each word token per label\n",
    "        \n",
    "        # pairWiseCountsMatched is a matrix of pairwise label co-incidence counts\n",
    "        # i.e. for every instance where label x and label y are both true, element [x,y] is incremented\n",
    "        # Also for every instance where label x and label y are both false, element [y,x] is incremented\n",
    "        # Hence one half of the matrix holds positive incidence of labels while the other half hold negative incidence.\n",
    "        self.pairwiseCountsMatched = np.zeros((len(classes), len(classes))) \n",
    "        \n",
    "        # pairwiseCountsMismatched is a matrix of pairwise label non-incidence counts\n",
    "        # i.e. for every instance where label x is true and label y in false, element [x,y] is incremented\n",
    "        # for every instance where label x is false and label y is true, element [y,x] is incremented\n",
    "        self.pairwiseCountsMismatched = np.zeros((len(classes), len(classes)))\n",
    "        \n",
    "        # Holds pairwise dependencies between classes.\n",
    "        # A True in element [x,y] instructs the classifier to compute P(x,y:X) during prediction.\n",
    "        # This is used to hopefully leverage correlations between classes to improve predictions.\n",
    "        self.pairwiseIndex = np.zeros((len(classes), len(classes)))\n",
    "        \n",
    "        # labelCounts holds the incidence count for every class.\n",
    "        # i.e. for every training instance labelled with x, labelCounts[x] is incremented\n",
    "        self.labelCounts = np.zeros(len(classes))\n",
    "        \n",
    "        # The number of training instances\n",
    "        self.documentCount = 0\n",
    "        \n",
    "    # Update counts with a new training example\n",
    "    def update(self, labels, tokens):\n",
    "        self.documentCount += 1\n",
    "        \n",
    "        for x in range(0, len(self.classes_)):\n",
    "            #print(x)\n",
    "            if labels[x]:\n",
    "                self.labelCounts[x] += 1\n",
    "                \n",
    "            for y in range(0, len(self.classes_)):\n",
    "                if y>x:\n",
    "                    if labels[x]:\n",
    "                        if labels[y]:\n",
    "                            self.pairwiseCountsMatched[x, y] += 1\n",
    "                        else:\n",
    "                            self.pairwiseCountsMismatched[x, y] += 1\n",
    "                else:\n",
    "                    if not labels[x]:\n",
    "                        if not labels[y]:\n",
    "                            self.pairwiseCountsMatched[x, y] += 1\n",
    "                        else:\n",
    "                            self.pairwiseCountsMismatched[x, y] += 1\n",
    "                        \n",
    "            \n",
    "        for w in tokens:\n",
    "            toAdd=0\n",
    "            # Does the word exist in our vocab?\n",
    "            if w >= len(self.globalCounts):\n",
    "                # No - extend the counts array to accomodate\n",
    "                toAdd = int(math.ceil((w-len(self.globalCounts)+1) / 1000.0) * 1000)\n",
    "                self.globalCounts = np.append(self.globalCounts, np.zeros(toAdd))\n",
    "\n",
    "                \n",
    "            self.globalCounts[w] += 1\n",
    "\n",
    "            if toAdd>0:\n",
    "                self.labelWordCounts = np.append(self.labelWordCounts, np.zeros((len(self.classes_),toAdd)), axis=1)\n",
    "\n",
    "            for x in range(0, len(self.classes_)):\n",
    "                if labels[x]:\n",
    "                    self.labelWordCounts[x, w] += 1                            \n",
    "\n",
    "    # Recalculate all probabilities with the current counts\n",
    "    def recalc(self):\n",
    "        #print(\"self.documentCount\", self.documentCount)\n",
    "        #print(\"self.labelCounts=\", self.labelCounts)\n",
    "        globalTotal = sum(self.globalCounts)\n",
    "        #print(globalTotal)\n",
    "        \n",
    "        classProbs = np.array(self.labelWordCounts)\n",
    "        #print(\"classProbs\", classProbs.shape)\n",
    "        #print(\"sum(classProbs)\", sum(sum(classProbs)))\n",
    "\n",
    "        #print(\"self.labelWordCounts=\", self.labelWordCounts+1)\n",
    "        #print(\"self.globalCounts=\", self.globalCounts+len(classProbs)+1)\n",
    "        \n",
    "        # Laplace smoothed word probability by emotion - p(w|e) \n",
    "        # P(w|e) = count(w when e)/count(w)\n",
    "        self.wordClassPosProbs = (1.0 * classProbs+1)/(self.globalCounts+len(classProbs))# p(x|y)\n",
    "\n",
    "        #print(\"self.wordClassPosProbs\", self.wordClassPosProbs.shape)\n",
    "        #print(\"self.wordClassPosProbs=\", self.wordClassPosProbs)\n",
    "        \n",
    "        # emotion probability - p(e) \n",
    "        # p(e) = count()\n",
    "        # sum classProbs per ROW to find total counts per class\n",
    "        \n",
    "        self.posClassProbs = (1.0 * self.labelCounts) / self.documentCount\n",
    "        #print(\"self.posClassProbs\", self.posClassProbs)\n",
    "        #print(\"self.posClassProbs\", self.posClassProbs.shape)\n",
    "        \n",
    "        classProbs = self.globalCounts - self.labelWordCounts#np.array(self.globalCounts) - classProbs\n",
    "        #print(\"classProbs=\", classProbs+1)\n",
    "        \n",
    "        # Laplace smoothed word probability by negative emotion - p(w|¬e)\n",
    "        self.wordClassNegProbs = (1.0 * classProbs+1)/(self.globalCounts+len(classProbs)+1)\n",
    "\n",
    "        #print(\"self.wordClassNegProbs\", self.wordClassNegProbs.shape)\n",
    "        #print(\"self.wordClassNegProbs=\", self.wordClassNegProbs)\n",
    "        \n",
    "        # negative emotion probability - p(¬e) - element per class\n",
    "        self.negClassProbs = (1.0 *(self.documentCount-self.labelCounts)) / self.documentCount\n",
    "        #print(\"self.negClassProbs\", self.negClassProbs)\n",
    "        #print(\"self.negClassProbs\", self.negClassProbs.shape)\n",
    "        \n",
    "        #print(self.posClassProbs + self.negClassProbs)\n",
    "        \n",
    "        # pairwise emotion conditional probability - p(e1 | e2) - class x class\n",
    "        self.pairwiseClassProbs_x_y_matched = np.zeros((len(self.classes_), len(self.classes_)))\n",
    "        for x in range(0, len(self.classes_)):\n",
    "            for y in range(0, len(self.classes_)):\n",
    "                if x==y:\n",
    "                    self.pairwiseClassProbs_x_y_matched[x, y]=1\n",
    "                else:\n",
    "                    if y > x: # p(x|y)\n",
    "                        self.pairwiseClassProbs_x_y_matched[x, y] = (1.0*self.pairwiseCountsMatched[x, y])/(sum(self.labelWordCounts[y]))\n",
    "                        self.pairwiseClassProbs_x_y_matched[y, x] = (1.0*self.pairwiseCountsMatched[y, x])/(sum(self.globalCounts)-sum(self.labelWordCounts[y]))\n",
    "                     \n",
    "        # p(¬e1|e1), p(e1|¬e2)                \n",
    "        self.pairwiseClassProbs_x_y_mismatched = np.zeros((len(self.classes_), len(self.classes_)))\n",
    "        for x in range(0, len(self.classes_)):\n",
    "            for y in range(0, len(self.classes_)):\n",
    "                if x==y:\n",
    "                    self.pairwiseClassProbs_x_y_mismatched[x, y]=0\n",
    "                else:\n",
    "                    if y > x:  \n",
    "                        self.pairwiseClassProbs_x_y_mismatched[x, y] = (1.0*self.pairwiseCountsMismatched[x, y])/(sum(self.labelWordCounts[y])) # p(x|¬y)\n",
    "                        self.pairwiseClassProbs_x_y_mismatched[y, x] = (1.0*self.pairwiseCountsMismatched[y, x])/(sum(self.globalCounts)-sum(self.labelWordCounts[y])) # p(¬x|y)\n",
    "\n",
    "        # pairwise emotion conditional probability - p(e2 | e1) - class x class\n",
    "        self.pairwiseClassProbs_y_x_matched = np.zeros((len(self.classes_), len(self.classes_)))\n",
    "        for x in range(0, len(self.classes_)):\n",
    "            for y in range(0, len(self.classes_)):\n",
    "                if x==y:\n",
    "                    self.pairwiseClassProbs_y_x_matched[x, y]=1\n",
    "                else:\n",
    "                    if y > x: # p(y|x)\n",
    "                        self.pairwiseClassProbs_y_x_matched[x, y] = (1.0*self.pairwiseCountsMatched[x, y])/(sum(self.labelWordCounts[x]))\n",
    "                        self.pairwiseClassProbs_y_x_matched[y, x] = (1.0*self.pairwiseCountsMatched[y, x])/(sum(self.globalCounts)-sum(self.labelWordCounts[x])) #p(¬y|¬x)\n",
    "        \n",
    "        # p(¬e2|e1), p(e2|¬e1)               \n",
    "        self.pairwiseClassProbs_y_x_mismatched = np.zeros((len(self.classes_), len(self.classes_)))\n",
    "        for x in range(0, len(self.classes_)):\n",
    "            for y in range(0, len(self.classes_)):\n",
    "                if x==y:\n",
    "                    self.pairwiseClassProbs_y_x_mismatched[x, y]=0\n",
    "                else:\n",
    "                    if y > x: # p(y|¬x)\n",
    "                        self.pairwiseClassProbs_y_x_mismatched[x, y] = (1.0*self.pairwiseCountsMismatched[x, y])/(sum(self.labelWordCounts[x]))\n",
    "                        self.pairwiseClassProbs_y_x_mismatched[y, x] = (1.0*self.pairwiseCountsMismatched[y, x])/(sum(self.globalCounts)-sum(self.labelWordCounts[x])) #\\p(¬y|x)\n",
    "                        \n",
    "    # set pairs of classes to correlate in results\n",
    "    # classPairs : list(tuple(_0, _1))\n",
    "    #\n",
    "    # self.pairwiseIndex = matrix(len(classes), len(classes))\n",
    "    def setPariwiseDependencies(self, classPairs):\n",
    "        self.pairwiseIndex = np.zeros((len(self.classes_), len(self.classes_)))\n",
    "        for pair in classPairs:\n",
    "            self.pairwiseIndex[pair[0], pair[1]] = True\n",
    "      \n",
    "    # Return the trained conditional probability of classes X and Y\n",
    "    # ------+--------------           \n",
    "    #       \\   x_pos\n",
    "    # y_pos | true | false  \n",
    "    # ------+------+-------\n",
    "    # true  | x|y  | ¬x|y \n",
    "    # false | x|¬y | ¬x|¬y\n",
    "    #-------+------+------\n",
    "    def getConditionalProbability(self, X, Y, x_pos, y_pos):\n",
    "        if Y > X:\n",
    "            if x_pos:\n",
    "                if not y_pos:\n",
    "                    #print \"#b\", key, value\n",
    "                    return self.pairwiseClassProbs_x_y_mismatched[X, Y]\n",
    "                else:\n",
    "                    return self.pairwiseClassProbs_x_y_matched[X, Y]\n",
    "\n",
    "            else:\n",
    "                if not y_pos:\n",
    "                    return self.pairwiseClassProbs_x_y_matched[Y, X]\n",
    "                else:\n",
    "                    return self.pairwiseClassProbs_x_y_mismatched[Y, X]\n",
    "        else:\n",
    "            if x_pos:\n",
    "                if not y_pos:\n",
    "                    return self.pairwiseClassProbs_y_x_mismatched[Y, X]\n",
    "                else:\n",
    "\n",
    "                    return self.pairwiseClassProbs_y_x_matched[Y, X]\n",
    "            else:\n",
    "                if not y_pos:\n",
    "                    return self.pairwiseClassProbs_y_x_matched[ X, Y] \n",
    "                else:\n",
    "                    return self.pairwiseClassProbs_y_x_mismatched[X, Y] \n",
    "\n",
    "    \n",
    "    # Classify a tokenised test instance\n",
    "    def classify(self, tokens):\n",
    "        # Predictions per class\n",
    "        classPredictions = [False] * len(self.classes_)\n",
    "        \n",
    "        # class probabilities; positive and negative\n",
    "        posClassProbs=[1.0]*len(classPredictions)\n",
    "        negClassProbs=[1.0]*len(classPredictions)\n",
    "        \n",
    "        # Compute P(Class|Token) and P(¬Class|Token)\n",
    "        for tok in tokens:\n",
    "            if tok < len(self.globalCounts):\n",
    "                posClassProbs *= self.wordClassPosProbs[:, tok]\n",
    "                negClassProbs *= self.wordClassNegProbs[:, tok]\n",
    "                \n",
    "        posClassProbs *= self.posClassProbs\n",
    "        negClassProbs *= self.negClassProbs\n",
    "        \n",
    "        preds = posClassProbs>negClassProbs\n",
    "\n",
    "        # Calculate pairwise probabilities\n",
    "        #p(X, Y | W) = p(X|Y).p(X).p(Y).Prod(w|X, w|Y)\n",
    "        for x in range(0, len(self.classes_)):\n",
    "            for y in range(0, len(self.classes_)):\n",
    "                if self.pairwiseIndex[x, y]:\n",
    "                    probs = np.zeros((2,2))\n",
    "\n",
    "                    probs[1, 1] = self.getConditionalProbability(x, y, True, True)\n",
    "                    probs[1, 0] = self.getConditionalProbability(x, y, True, False)\n",
    "                    probs[0, 1] = self.getConditionalProbability(x, y, False, True)\n",
    "                    probs[0, 0] = self.getConditionalProbability(x, y, False, False)\n",
    "\n",
    "                    probs[1,:] *= posClassProbs[x]\n",
    "                    probs[0,:] *= negClassProbs[x]\n",
    "                    probs[:,1] *= posClassProbs[y]\n",
    "                    probs[:,0] *= negClassProbs[y]\n",
    "\n",
    "                    argmax = np.argmax(probs)\n",
    "                    argmax_prob = (argmax/probs.shape[1], argmax % probs.shape[1])\n",
    "\n",
    "                    preds[x] = bool(argmax_prob[0])\n",
    "                    preds[y] = bool(argmax_prob[1])\n",
    "\n",
    "        \n",
    "        return preds\n",
    "                    \n",
    "        \n",
    "    classes_=list()\n",
    "    globalCounts=np.array([]) # array of counts indexed by token id\n",
    "    labelWordCounts=np.array([]) # array of classes_ rows x tokens columns\n",
    "    # array classes x classes x tokens\n",
    "    # for y>x (top diagonal half) contains counts where sentiment(x) and sentiment(y)\n",
    "    # for x<y (bottom diagonal half) contains counts where not(sentiment(x)) and not(sentiment(y))\n",
    "    pairwiseCountsMatched=np.array([]) \n",
    "    \n",
    "    # array classes x classes x tokens\n",
    "    # for y>x (top diagonal half) contains counts where sentiment(x) and not(sentiment(y))\n",
    "    # for x<y (bottom diagonal half) contains counts where not(sentiment(x)) and sentiment(y)\n",
    "    pairwiseCountsMismatched=np.array([]) \n",
    "    classPairs=dict() # dict of emotion id -> emotion id\n",
    "    pairwiseIndex=np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class ModelScores calculates and maintains prediction scores for a classificiation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelScores:\n",
    "    def __init__(self, knownClasses):\n",
    "        self.knownClasses = knownClasses\n",
    "        self.truePositives = np.zeros(len(knownClasses))\n",
    "        self.trueNegatives = np.zeros(len(knownClasses))\n",
    "        self.falsePositives = np.zeros(len(knownClasses))\n",
    "        self.falseNegatives = np.zeros(len(knownClasses))\n",
    "        \n",
    "    def accumulate(self, predictedLabels, trainingLabels):\n",
    "        self.truePositives += np.logical_and(predictedLabels, trainingLabels)\n",
    "        self.trueNegatives += np.logical_and(np.logical_not(predictedLabels), np.logical_not(trainingLabels))\n",
    "        self.falsePositives += np.logical_and(preds, np.logical_not(trainingLabels))\n",
    "        self.falseNegatives += np.logical_and(np.logical_not(preds), trainingLabels)\n",
    "\n",
    "    def getStats(self):\n",
    "        precision = self.truePositives / (self.truePositives+self.falsePositives)\n",
    "        recall = self.truePositives / (self.truePositives+self.falseNegatives)\n",
    "        accuracy = (self.truePositives+self.trueNegatives)/(self.truePositives+self.trueNegatives+self.falsePositives+self.falseNegatives)\n",
    "        \n",
    "        f1 = 2* ((precision * recall)/(precision + recall))\n",
    "        \n",
    "        return (accuracy, precision, recall, f1)\n",
    "    \n",
    "    def printScores(self):\n",
    "        (accuracy, precision, recall, f1score) = self.getStats()\n",
    "        print self.knownClasses\n",
    "        print \"true positives = \", self.truePositives\n",
    "        print \"true negatives = \", self.trueNegatives\n",
    "        print \"false positives = \", self.falsePositives\n",
    "        print \"false negatives = \", self.falseNegatives\n",
    "        print\n",
    "        print \"accuracy=\", np.round(accuracy, 3)\n",
    "        print \"precision=\", np.round(precision, 3)\n",
    "        print \"recall=\", np.round(recall, 3)\n",
    "        print \"f1score=\", np.round(f1score, 3)\n",
    "      \n",
    "    def printScoresAsTable(self):\n",
    "        (accuracy, precision, recall, f1score) = self.getStats()\n",
    "        return pd.DataFrame(data=[self.truePositives, self.trueNegatives, \\\n",
    "                                  self.falsePositives, self.falseNegatives, \\\n",
    "                                  np.round(accuracy,3), np.round(precision, 3), \\\n",
    "                                  np.round(recall,3), np.round(f1score,3)],\n",
    "                    index=[\"True Positives\", \"True Negatives\", \\\n",
    "                           \"False Positives\", \"False Negatives\", \\\n",
    "                           \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"],\n",
    "                    columns=self.knownClasses)\n",
    "\n",
    "    def printScoresAsTableWithDeltas(self, other):\n",
    "        (accuracy, precision, recall, f1score) = self.getStats()\n",
    "        (da, dp, dr, df1) = self.getDeltas(other)\n",
    "        \n",
    "        addDelta = lambda a, d: np.array([str(x)+\"(\"+str(y)+\")\" for(x, y) in zip(a, d)])\n",
    "        data=[self.truePositives, self.trueNegatives, \\\n",
    "                                  self.falsePositives, self.falseNegatives, \\\n",
    "                                  addDelta(np.round(accuracy,3), np.round(da,3)), \n",
    "                                  addDelta(np.round(precision, 3), np.round(dp, 3)), \\\n",
    "                                  addDelta(np.round(recall,3), np.round(dr, 3)), \\\n",
    "                                  addDelta(np.round(f1score,3), np.round(df1, 3))]\n",
    "        \n",
    "        print data\n",
    "        return pd.DataFrame(data=data,\n",
    "                    index=[\"True Positives\", \"True Negatives\", \\\n",
    "                           \"False Positives\", \"False Negatives\", \\\n",
    "                           \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"],\n",
    "                    columns=self.knownClasses)\n",
    "    \n",
    "    def getDeltas(self, other):\n",
    "        (o_accuracy, o_precision, o_recall, o_f1) = other.getStats()\n",
    "        (accuracy, precision, recall, f1) = self.getStats()\n",
    "        \n",
    "        return (accuracy-o_accuracy, precision-o_precision, recall-o_recall, f1-o_f1)\n",
    "    \n",
    "    def printDeltas(self, other):\n",
    "        (accuracy, precision, recall, f1) = self.getDeltas(other)\n",
    "        \n",
    "        print \"delta accuracy=\", np.round(accuracy, 3)\n",
    "        print \"delta precision=\", np.round(precision, 3)\n",
    "        print \"delta recall=\", np.round(recall, 3)\n",
    "        print \"delta f1score=\", np.round(f1, 3)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Individual Emotions\n",
    "\n",
    "Predict individual emotions based on the training data. \n",
    "\n",
    "No dependencies between different emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising NaiveBayes1 class with  8  classes...\n",
      "Done. Processed  2914  instances\n"
     ]
    }
   ],
   "source": [
    "bow = BoW() # Create empty Bag of Words\n",
    "nb1 = NaiveBayes1(KnownSentiments) # Create Naive Bayes class with the known sentiments as labels.\n",
    "i=0\n",
    "# Process the training file line-by-line\n",
    "for line in open(trainFile):\n",
    "    #print i, line\n",
    "    (classMap, tokens)=parseSentence(line) # Parse each sentence returning sentiments and tokens\n",
    "    #print(i, \": \", classMap)\n",
    "    if len(classMap)==0:\n",
    "        i+=1\n",
    "        continue\n",
    "        \n",
    "    token_ndx = bow.fit_transform(tokens) # Transform tokens to term-document frequency\n",
    "    \n",
    "    nb1.update(classMap, token_ndx) # Update the Naive Bayes class with the new training data\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "nb1.recalc() # Recalculate the probabilities\n",
    "print \"Done. Processed \", i, \" instances\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anger', 'Anticipation', 'Disgust', 'Fear', 'Joy', 'Sadness', 'Surprise', 'Trust']\n",
      "true positives =  [1176.  977.  477.  105.  353.  956.    2.  115.]\n",
      "true negatives =  [ 180.  250.  777. 1099.  938.  200. 1426. 1233.]\n",
      "false positives =  [531. 501. 267.  57. 261. 695.   3.  42.]\n",
      "false negatives =  [ 69. 228. 435. 695. 404. 105. 525. 566.]\n",
      "\n",
      "accuracy= [0.693 0.627 0.641 0.616 0.66  0.591 0.73  0.689]\n",
      "precision= [0.689 0.661 0.641 0.648 0.575 0.579 0.4   0.732]\n",
      "recall= [0.945 0.811 0.523 0.131 0.466 0.901 0.004 0.169]\n",
      "f1score= [0.797 0.728 0.576 0.218 0.515 0.705 0.008 0.274]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Positives</th>\n",
       "      <td>1176.000</td>\n",
       "      <td>977.000</td>\n",
       "      <td>477.000</td>\n",
       "      <td>105.000</td>\n",
       "      <td>353.000</td>\n",
       "      <td>956.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>115.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Negatives</th>\n",
       "      <td>180.000</td>\n",
       "      <td>250.000</td>\n",
       "      <td>777.000</td>\n",
       "      <td>1099.000</td>\n",
       "      <td>938.000</td>\n",
       "      <td>200.000</td>\n",
       "      <td>1426.000</td>\n",
       "      <td>1233.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positives</th>\n",
       "      <td>531.000</td>\n",
       "      <td>501.000</td>\n",
       "      <td>267.000</td>\n",
       "      <td>57.000</td>\n",
       "      <td>261.000</td>\n",
       "      <td>695.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>42.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negatives</th>\n",
       "      <td>69.000</td>\n",
       "      <td>228.000</td>\n",
       "      <td>435.000</td>\n",
       "      <td>695.000</td>\n",
       "      <td>404.000</td>\n",
       "      <td>105.000</td>\n",
       "      <td>525.000</td>\n",
       "      <td>566.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.693</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.689</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.945</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-Score</th>\n",
       "      <td>0.797</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Anger  Anticipation  Disgust      Fear      Joy  Sadness  \\\n",
       "True Positives   1176.000       977.000  477.000   105.000  353.000  956.000   \n",
       "True Negatives    180.000       250.000  777.000  1099.000  938.000  200.000   \n",
       "False Positives   531.000       501.000  267.000    57.000  261.000  695.000   \n",
       "False Negatives    69.000       228.000  435.000   695.000  404.000  105.000   \n",
       "Accuracy            0.693         0.627    0.641     0.616    0.660    0.591   \n",
       "Precision           0.689         0.661    0.641     0.648    0.575    0.579   \n",
       "Recall              0.945         0.811    0.523     0.131    0.466    0.901   \n",
       "F1-Score            0.797         0.728    0.576     0.218    0.515    0.705   \n",
       "\n",
       "                 Surprise     Trust  \n",
       "True Positives      2.000   115.000  \n",
       "True Negatives   1426.000  1233.000  \n",
       "False Positives     3.000    42.000  \n",
       "False Negatives   525.000   566.000  \n",
       "Accuracy            0.730     0.689  \n",
       "Precision           0.400     0.732  \n",
       "Recall              0.004     0.169  \n",
       "F1-Score            0.008     0.274  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb1.setPariwiseDependencies({}) # Assume NO pairwise dependencies between labels\n",
    "origScores = ModelScores(KnownSentiments)\n",
    "\n",
    "# Process the test file\n",
    "for line in open(testFile):\n",
    "    # Parse, tokenise and BoW...\n",
    "    (testClassMap, tokens) = parseSentence(line)\n",
    "    if (tokens is None):\n",
    "        continue\n",
    "    tokens = bow.transform(tokens)\n",
    "    # ... and use Naive Bayes to classify\n",
    "    preds = nb1.classify(tokens)\n",
    "     \n",
    "    origScores.accumulate(preds, testClassMap)\n",
    "\n",
    "origScores.printScores()\n",
    "(orig_accuracy, orig_precision, orig_recall, orig_f1) = origScores.getStats()\n",
    "origScores.printScoresAsTable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, there are a couple of problem points in our predictions. Sadness and Joy have very low recalls, resulting in very low f1 scores. Accuracy is good all around, however accuracy alone as a performance indicator is misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Manual addition of relationships between emotion labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2a: Pariwise combinations of 2 emotion labels.\n",
    "\n",
    "We notice from the above scores that while Sadness has the worst accuracy, Surprise, however has by far the worst f1-score, due to a very low recall. In fact, our classifier seems very reluctant to classify Sadness, having only scored 2 true positives on this label, compared to 525 false negatives.\n",
    "\n",
    "We tried to improve the score on the Surprise label by adding a relationship to another emotion and we found that pairing Surprise to Anger gave the best, albeit modest, improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etienne/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E1 f1-Score', 'E1 f1-delta', 'E2 f1-Score', 'E2 f1-delta']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1 f1-Score</th>\n",
       "      <th>E1 f1-delta</th>\n",
       "      <th>E2 f1-Score</th>\n",
       "      <th>E2 f1-delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Surprise-&gt;Anger</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770719</td>\n",
       "      <td>-0.026029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surprise-&gt;Anticipation</th>\n",
       "      <td>0.011257</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.729174</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surprise-&gt;Disgust</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.480638</td>\n",
       "      <td>-0.095449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surprise-&gt;Fear</th>\n",
       "      <td>0.007533</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.095462</td>\n",
       "      <td>-0.122834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surprise-&gt;Joy</th>\n",
       "      <td>0.003781</td>\n",
       "      <td>-0.003738</td>\n",
       "      <td>0.470999</td>\n",
       "      <td>-0.043953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surprise-&gt;Sadness</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.682737</td>\n",
       "      <td>-0.022278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surprise-&gt;Trust</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.154037</td>\n",
       "      <td>-0.120426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anger-&gt;Surprise</th>\n",
       "      <td>0.777046</td>\n",
       "      <td>-0.019702</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anticipation-&gt;Surprise</th>\n",
       "      <td>0.755772</td>\n",
       "      <td>0.027483</td>\n",
       "      <td>0.011257</td>\n",
       "      <td>0.003738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disgust-&gt;Surprise</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.023913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fear-&gt;Surprise</th>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.026149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joy-&gt;Surprise</th>\n",
       "      <td>0.587016</td>\n",
       "      <td>0.072063</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>-0.003738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sadness-&gt;Surprise</th>\n",
       "      <td>0.700569</td>\n",
       "      <td>-0.004446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trust-&gt;Surprise</th>\n",
       "      <td>0.289885</td>\n",
       "      <td>0.015422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        E1 f1-Score  E1 f1-delta  E2 f1-Score  E2 f1-delta\n",
       "Surprise->Anger                 NaN          NaN     0.770719    -0.026029\n",
       "Surprise->Anticipation     0.011257     0.003738     0.729174     0.000885\n",
       "Surprise->Disgust               NaN          NaN     0.480638    -0.095449\n",
       "Surprise->Fear             0.007533     0.000014     0.095462    -0.122834\n",
       "Surprise->Joy              0.003781    -0.003738     0.470999    -0.043953\n",
       "Surprise->Sadness               NaN          NaN     0.682737    -0.022278\n",
       "Surprise->Trust                 NaN          NaN     0.154037    -0.120426\n",
       "Anger->Surprise            0.777046    -0.019702          NaN          NaN\n",
       "Anticipation->Surprise     0.755772     0.027483     0.011257     0.003738\n",
       "Disgust->Surprise          0.600000     0.023913          NaN          NaN\n",
       "Fear->Surprise             0.244444     0.026149          NaN          NaN\n",
       "Joy->Surprise              0.587016     0.072063     0.003781    -0.003738\n",
       "Sadness->Surprise          0.700569    -0.004446          NaN          NaN\n",
       "Trust->Surprise            0.289885     0.015422          NaN          NaN"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Anger | Sadness\n",
    "indexes=[]\n",
    "columns=[\"E1 f1-Score\", \"E1 f1-delta\", \"E2 f1-Score\", \"E2 f1-delta\"]\n",
    "f1scores=np.zeros(((len(KnownSentiments)-1)*2, 4))\n",
    "\n",
    "def validate(e1, e2):\n",
    "    indexes.extend([KnownSentiments[e1]+\"->\"+KnownSentiments[e2]])\n",
    "    nb1.setPariwiseDependencies([(e1,e2)])\n",
    "    scores = ModelScores(KnownSentiments)\n",
    "\n",
    "    for line in open(testFile):\n",
    "        (testClassMap, tokens) = parseSentence(line)\n",
    "        if (tokens is None):\n",
    "            continue\n",
    "        tokens = bow.transform(tokens)\n",
    "        preds = nb1.classify(tokens)\n",
    "\n",
    "        scores.accumulate(preds, testClassMap)\n",
    "\n",
    "    (a, r, p, f1) = scores.getStats()\n",
    "    (da, dr, dp, df1) = scores.getDeltas(origScores)\n",
    "    f1scores[i, 0] = f1[e1]\n",
    "    f1scores[i, 1] = df1[e1]\n",
    "    f1scores[i, 2] = f1[e2]\n",
    "    f1scores[i, 3] = df1[e2]\n",
    "\n",
    "i=0\n",
    "for e in range(0, 8):\n",
    "    if e!=6:\n",
    "        validate(6, e)\n",
    "        i+=1\n",
    "        \n",
    "for e in range(0,8):\n",
    "    if e!=6:\n",
    "        validate(e, 6)\n",
    "        i+=1\n",
    "        \n",
    "        #scores.printScores()\n",
    "\n",
    "        #scores.printDeltas(origScores)\n",
    "print columns\n",
    "f1scores_df = pd.DataFrame(data=f1scores, columns=columns, index=indexes)\n",
    "\n",
    "f1scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2b - Best set of pairwise variable assignments\n",
    "We now want to search through all pairwise combinations of emotions and all four combinations of conditional probability assignments (x|y, x|¬y, ¬x|y, ¬x|¬y) to find the best assignments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating P( Anger , Anticipation  | X)\n",
      "(-0.001642360451014624, ':', -0.0016423604510146378)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47602870374123296 ( -0.001642360451014624 )\n",
      "Evaluating P( Anger , Disgust  | X)\n",
      "(0.006635081319519287, ':', 0.006635081319519287)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4843061455117669 ( 0.006635081319519287 )\n",
      "Evaluating P( Anger , Fear  | X)\n",
      "(-0.0050065677835374744, ':', -0.005006567783537461)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4726644964087101 ( -0.0050065677835374744 )\n",
      "Evaluating P( Anger , Joy  | X)\n",
      "(-0.007342081390642297, ':', -0.0073420813906423035)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4703289828016053 ( -0.007342081390642297 )\n",
      "Evaluating P( Anger , Sadness  | X)\n",
      "(0.0009198558938562362, ':', 0.0009198558938562224)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4785909200861038 ( 0.0009198558938562362 )\n",
      "Evaluating P( Anger , Surprise  | X)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etienne/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Anger , Trust  | X)\n",
      "(-0.005444459350626429, ':', -0.005444459350626412)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47222660484162116 ( -0.005444459350626429 )\n",
      "Evaluating P( Anticipation , Anger  | X)\n",
      "(-0.005093717398842568, ':', -0.005093717398842623)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.472577346793405 ( -0.005093717398842568 )\n",
      "Evaluating P( Anticipation , Disgust  | X)\n",
      "(-0.0057622878853967885, ':', -0.005762287885396858)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4719087763068508 ( -0.0057622878853967885 )\n",
      "Evaluating P( Anticipation , Fear  | X)\n",
      "(-0.0067718454252605476, ':', -0.006771845425260523)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47089921876698704 ( -0.0067718454252605476 )\n",
      "Evaluating P( Anticipation , Joy  | X)\n",
      "(0.0014774749586726754, ':', 0.0014774749586727032)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47914853915092026 ( 0.0014774749586726754 )\n",
      "Evaluating P( Anticipation , Sadness  | X)\n",
      "(-0.0016019098937857246, ':', -0.0016019098937857384)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47606915429846186 ( -0.0016019098937857246 )\n",
      "Evaluating P( Anticipation , Surprise  | X)\n",
      "(0.0046823278343668395, ':', 0.004682327834366804)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4823533920266144 ( 0.0046823278343668395 )\n",
      "Evaluating P( Anticipation , Trust  | X)\n",
      "(-0.0010874096842085623, ':', -0.0010874096842085484)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.476583654508039 ( -0.0010874096842085623 )\n",
      "Evaluating P( Disgust , Anger  | X)\n",
      "(0.003414307927609883, ':', 0.003414307927609911)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.48108537211985747 ( 0.003414307927609883 )\n",
      "Evaluating P( Disgust , Anticipation  | X)\n",
      "(-0.007552182572768129, ':', -0.007552182572768129)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47011888161947946 ( -0.007552182572768129 )\n",
      "Evaluating P( Disgust , Fear  | X)\n",
      "(-0.006411176284998876, ':', -0.006411176284998949)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4712598879072487 ( -0.006411176284998876 )\n",
      "Evaluating P( Disgust , Joy  | X)\n",
      "(0.004879523852727385, ':', 0.004879523852727385)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.48255058804497497 ( 0.004879523852727385 )\n",
      "Evaluating P( Disgust , Sadness  | X)\n",
      "(0.00375586807822359, ':', 0.00375586807822359)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4814269322704712 ( 0.00375586807822359 )\n",
      "Evaluating P( Disgust , Surprise  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Disgust , Trust  | X)\n",
      "(0.010669109815159339, ':', 0.010669109815159304)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4883401740074069 ( 0.010669109815159339 )\n",
      "Evaluating P( Fear , Anger  | X)\n",
      "(-0.012010116541837368, ':', -0.012010116541837354)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4656609476504102 ( -0.012010116541837368 )\n",
      "Evaluating P( Fear , Anticipation  | X)\n",
      "(-0.009045242807997234, ':', -0.00904524280799724)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.46862582138425035 ( -0.009045242807997234 )\n",
      "Evaluating P( Fear , Disgust  | X)\n",
      "(-0.013659787400198908, ':', -0.013659787400198978)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4640112767920487 ( -0.013659787400198908 )\n",
      "Evaluating P( Fear , Joy  | X)\n",
      "(-0.0010853923806166677, ':', -0.0010853923806167058)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4765856718116309 ( -0.0010853923806166677 )\n",
      "Evaluating P( Fear , Sadness  | X)\n",
      "(0.005806208443920657, ':', 0.005806208443920616)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.48347727263616824 ( 0.005806208443920657 )\n",
      "Evaluating P( Fear , Surprise  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Fear , Trust  | X)\n",
      "(-0.0011371795675763074, ':', -0.0011371795675763074)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4765338846246713 ( -0.0011371795675763074 )\n",
      "Evaluating P( Joy , Anger  | X)\n",
      "(-0.012662592565158559, ':', -0.012662592565158573)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.465008471627089 ( -0.012662592565158559 )\n",
      "Evaluating P( Joy , Anticipation  | X)\n",
      "(-0.0017452564916597413, ':', -0.001745256491659783)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47592580770058784 ( -0.0017452564916597413 )\n",
      "Evaluating P( Joy , Disgust  | X)\n",
      "(0.0043465603982680845, ':', 0.004346560398268029)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.48201762459051567 ( 0.0043465603982680845 )\n",
      "Evaluating P( Joy , Fear  | X)\n",
      "(3.636806252704794e-06, ':', 3.6368062526562217e-06)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4776747009985003 ( 3.636806252704794e-06 )\n",
      "Evaluating P( Joy , Sadness  | X)\n",
      "(-0.00685999939654669, ':', -0.006859999396546711)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4708110647957009 ( -0.00685999939654669 )\n",
      "Evaluating P( Joy , Surprise  | X)\n",
      "(0.0072825240220578125, ':', 0.007282524022057778)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4849535882143054 ( 0.0072825240220578125 )\n",
      "Evaluating P( Joy , Trust  | X)\n",
      "(-4.8637638699278174e-05, ':', -4.863763869929899e-05)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4776224265535483 ( -4.8637638699278174e-05 )\n",
      "Evaluating P( Sadness , Anger  | X)\n",
      "(0.0006183339132306287, ':', 0.0006183339132306287)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4782893981054782 ( 0.0006183339132306287 )\n",
      "Evaluating P( Sadness , Anticipation  | X)\n",
      "(-0.0002247792900340606, ':', -0.0002247792900340606)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4774462849022135 ( -0.0002247792900340606 )\n",
      "Evaluating P( Sadness , Disgust  | X)\n",
      "(0.006092191324433505, ':', 0.006092191324433421)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4837632555166811 ( 0.006092191324433505 )\n",
      "Evaluating P( Sadness , Fear  | X)\n",
      "(0.02070378128161554, ':', 0.02070378128161543)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4983748454738631 ( 0.02070378128161554 )\n",
      "Evaluating P( Sadness , Joy  | X)\n",
      "(-0.0032443815873469672, ':', -0.0032443815873470228)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4744266826049006 ( -0.0032443815873469672 )\n",
      "Evaluating P( Sadness , Surprise  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Sadness , Trust  | X)\n",
      "(-0.006699677028158102, ':', -0.0066996770281580985)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4709713871640895 ( -0.006699677028158102 )\n",
      "Evaluating P( Surprise , Anger  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Surprise , Anticipation  | X)\n",
      "(0.0014432361233498447, ':', 0.0014432361233498135)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47911430031559743 ( 0.0014432361233498447 )\n",
      "Evaluating P( Surprise , Disgust  | X)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etienne/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Surprise , Fear  | X)\n",
      "(-0.010113478863478798, ':', -0.010113478863478867)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4675575853287688 ( -0.010113478863478798 )\n",
      "Evaluating P( Surprise , Joy  | X)\n",
      "(-0.0010597838473834775, ':', -0.0010597838473834706)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4766112803448641 ( -0.0010597838473834775 )\n",
      "Evaluating P( Surprise , Sadness  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Surprise , Trust  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n",
      "Evaluating P( Trust , Anger  | X)\n",
      "(-0.004941079993604491, ':', -0.004941079993604481)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4727299841986431 ( -0.004941079993604491 )\n",
      "Evaluating P( Trust , Anticipation  | X)\n",
      "(-0.005751196035703676, ':', -0.005751196035703659)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4719198681565439 ( -0.005751196035703676 )\n",
      "Evaluating P( Trust , Disgust  | X)\n",
      "(0.009642641338028668, ':', 0.009642641338028633)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.48731370553027625 ( 0.009642641338028668 )\n",
      "Evaluating P( Trust , Fear  | X)\n",
      "(-0.004685918133076505, ':', -0.004685918133076537)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.4729851460591711 ( -0.004685918133076505 )\n",
      "Evaluating P( Trust , Joy  | X)\n",
      "(-0.006215534385696053, ':', -0.006215534385696102)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47145552980655153 ( -0.006215534385696053 )\n",
      "Evaluating P( Trust , Sadness  | X)\n",
      "(-0.007586595803451557, ':', -0.007586595803451581)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  0.47008446838879603 ( -0.007586595803451557 )\n",
      "Evaluating P( Trust , Surprise  | X)\n",
      "(nan, ':', nan)\n",
      "    Edge changed mean f1-score from  0.4776710641922476  to  nan ( nan )\n"
     ]
    }
   ],
   "source": [
    "# Save the original performance metrics for the unbiased naive bayes model so that we can\n",
    "# determine which pairwise assignments resulted in improves predictive power.\n",
    "(origAccuracy, origPrecision, origRecall, origf1) = origScores.getStats()\n",
    "mean_orig_f1 = np.mean(origf1) # Mean f1-score for ubnbiased model\n",
    "\n",
    "# Store the f1 score deltas for every pairwise combination\n",
    "pairwiseImprovements = np.zeros((len(KnownSentiments), len(KnownSentiments)))\n",
    "\n",
    "\n",
    "for x in range(0, len(KnownSentiments)):\n",
    "    for y in range(0, len(KnownSentiments)):\n",
    "        if x!=y:\n",
    "            sentx = KnownSentiments[x]\n",
    "            senty = KnownSentiments[y]                \n",
    "                \n",
    "            print \"Evaluating P(\", sentx, \",\", senty, \" | X)\"\n",
    "            \n",
    "            # Set the dependency in the classifier\n",
    "            nb1.setPariwiseDependencies([(x, y)])\n",
    "\n",
    "            # Construct a ModelScores object to hold the new scores\n",
    "            scores = ModelScores(KnownSentiments)\n",
    "\n",
    "            # Process the test file, classifying each instance and accumulated\n",
    "            # the result in our ModelScores object\n",
    "            for line in open(testFile):\n",
    "                (testClassMap, tokens) = parseSentence(line)\n",
    "                if (tokens is None):\n",
    "                    continue\n",
    "                tokens = bow.transform(tokens)\n",
    "                preds = nb1.classify(tokens)\n",
    "\n",
    "                scores.accumulate(preds, testClassMap)\n",
    "\n",
    "            # Retrieve the new scores...\n",
    "            (a, p, r, f1) = scores.getStats()\n",
    "            # ... and calculate how much they've changed from the unbiased model\n",
    "            (d_a, d_p, d_r, d_f1) = scores.getDeltas(origScores)\n",
    "            \n",
    "            mean_f1 = np.mean(f1) # Mean f1-score of biased model\n",
    "            mean_delta_f1 = mean_f1 - mean_orig_f1 # Change in f1-score\n",
    "            print(mean_delta_f1, \":\", np.mean(d_f1))\n",
    "            #deltas[combination, y] = d_a[x]\n",
    "            print \"    Edge changed mean f1-score from \", mean_orig_f1, \" to \", mean_f1, \"(\" , (mean_f1-mean_orig_f1), \")\"\n",
    "            pairwiseImprovements[x, y] = mean_delta_f1 # Save the delta\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the beneficial assignments discovered in the previous cell and configure a new classifier with the best ones that don't conflict.\n",
    "\n",
    "i.e. if a beneficial relationship is Anger|Fear and we configure it into the  new classifier, we cannot configure the classifier with any other relationships involving Anger or Fear since we are only considering pairwise relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.   , -0.002,  0.007, -0.005, -0.007,  0.001,    nan, -0.005],\n",
       "       [-0.005,  0.   , -0.006, -0.007,  0.001, -0.002,  0.005, -0.001],\n",
       "       [ 0.003, -0.008,  0.   , -0.006,  0.005,  0.004,    nan,  0.011],\n",
       "       [-0.012, -0.009, -0.014,  0.   , -0.001,  0.006,    nan, -0.001],\n",
       "       [-0.013, -0.002,  0.004,  0.   ,  0.   , -0.007,  0.007, -0.   ],\n",
       "       [ 0.001, -0.   ,  0.006,  0.021, -0.003,  0.   ,    nan, -0.007],\n",
       "       [   nan,  0.001,    nan, -0.01 , -0.001,    nan,  0.   ,    nan],\n",
       "       [-0.005, -0.006,  0.01 , -0.005, -0.006, -0.008,    nan,  0.   ]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(pairwiseImprovements, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beneficial Edge:  Sadness | Fear : f1-score + 0.02070378128161554\n",
      "(5, 3)\n",
      "Beneficial Edge:  Disgust | Trust : f1-score + 0.010669109815159339\n",
      "(2, 7)\n",
      "Beneficial Edge:  Trust | Disgust : f1-score + 0.009642641338028668\n",
      "Beneficial Edge:  Joy | Surprise : f1-score + 0.0072825240220578125\n",
      "(4, 6)\n",
      "Beneficial Edge:  Anger | Disgust : f1-score + 0.006635081319519287\n",
      "Beneficial Edge:  Sadness | Disgust : f1-score + 0.006092191324433505\n",
      "Beneficial Edge:  Fear | Sadness : f1-score + 0.005806208443920657\n",
      "Beneficial Edge:  Disgust | Joy : f1-score + 0.004879523852727385\n",
      "Beneficial Edge:  Anticipation | Surprise : f1-score + 0.0046823278343668395\n",
      "Beneficial Edge:  Joy | Disgust : f1-score + 0.0043465603982680845\n",
      "Beneficial Edge:  Disgust | Sadness : f1-score + 0.00375586807822359\n",
      "Beneficial Edge:  Disgust | Anger : f1-score + 0.003414307927609883\n",
      "Beneficial Edge:  Anticipation | Joy : f1-score + 0.0014774749586726754\n",
      "Beneficial Edge:  Surprise | Anticipation : f1-score + 0.0014432361233498447\n",
      "Beneficial Edge:  Anger | Sadness : f1-score + 0.0009198558938562362\n",
      "Beneficial Edge:  Sadness | Anger : f1-score + 0.0006183339132306287\n",
      "Beneficial Edge:  Joy | Fear : f1-score + 3.636806252704794e-06\n"
     ]
    }
   ],
   "source": [
    "# Find the assignments that gave the best improvements\n",
    "sortedNdxs = np.argsort(pairwiseImprovements.flatten())[::-1]\n",
    "usedLabels = set()\n",
    "globalPariwiseAssignments = list()\n",
    "\n",
    "for ndx in sortedNdxs:\n",
    "    #print ndx\n",
    "    x = ndx / len(KnownSentiments)\n",
    "    y = ndx % len(KnownSentiments)\n",
    "   # print(x, y)\n",
    "    if pairwiseImprovements[x, y]>0:\n",
    "        print \"Beneficial Edge: \", KnownSentiments[x], \"|\", KnownSentiments[y], \": f1-score +\", pairwiseImprovements[x, y]\n",
    "        \n",
    "        if (x not in usedLabels) and (y not in usedLabels):\n",
    "            usedLabels.update([x])\n",
    "            usedLabels.update([y])\n",
    "            print(x, y)\n",
    "            globalPariwiseAssignments.extend([(x, y)])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now re-classify the test set using the discovered relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 3), (2, 7), (4, 6)]\n"
     ]
    }
   ],
   "source": [
    "print globalPariwiseAssignments\n",
    "\n",
    "nb1.setPariwiseDependencies(globalPariwiseAssignments)\n",
    "\n",
    "scores = ModelScores(KnownSentiments)\n",
    "\n",
    "for line in open(testFile):\n",
    "    (testClassMap, tokens) = parseSentence(line)\n",
    "    if (tokens is None):\n",
    "        continue\n",
    "    tokens = bow.transform(tokens)\n",
    "    preds = nb1.classify(tokens)\n",
    "     \n",
    "    scores.accumulate(preds, testClassMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.176e+03, 9.770e+02, 6.320e+02, 2.550e+02, 5.380e+02, 7.980e+02,\n",
      "       1.000e+00, 1.300e+02]), array([ 180.,  250.,  584.,  976.,  629.,  393., 1429., 1234.]), array([531., 501., 460., 180., 570., 502.,   0.,  41.]), array([ 69., 228., 280., 545., 219., 263., 526., 551.]), array(['0.693(0.0)', '0.627(0.0)', '0.622(-0.019)', '0.629(0.014)',\n",
      "       '0.597(-0.063)', '0.609(0.018)', '0.731(0.001)', '0.697(0.008)'],\n",
      "      dtype='|S13'), array(['0.689(0.0)', '0.661(0.0)', '0.579(-0.062)', '0.586(-0.062)',\n",
      "       '0.486(-0.089)', '0.614(0.035)', '1.0(0.6)', '0.76(0.028)'],\n",
      "      dtype='|S13'), array(['0.945(0.0)', '0.811(0.0)', '0.693(0.17)', '0.319(0.187)',\n",
      "       '0.711(0.244)', '0.752(-0.149)', '0.002(-0.002)', '0.191(0.022)'],\n",
      "      dtype='|S13'), array(['0.797(0.0)', '0.728(0.0)', '0.631(0.055)', '0.413(0.195)',\n",
      "       '0.577(0.062)', '0.676(-0.029)', '0.004(-0.004)', '0.305(0.031)'],\n",
      "      dtype='|S13')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Positives</th>\n",
       "      <td>1176</td>\n",
       "      <td>977</td>\n",
       "      <td>632</td>\n",
       "      <td>255</td>\n",
       "      <td>538</td>\n",
       "      <td>798</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True Negatives</th>\n",
       "      <td>180</td>\n",
       "      <td>250</td>\n",
       "      <td>584</td>\n",
       "      <td>976</td>\n",
       "      <td>629</td>\n",
       "      <td>393</td>\n",
       "      <td>1429</td>\n",
       "      <td>1234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Positives</th>\n",
       "      <td>531</td>\n",
       "      <td>501</td>\n",
       "      <td>460</td>\n",
       "      <td>180</td>\n",
       "      <td>570</td>\n",
       "      <td>502</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>False Negatives</th>\n",
       "      <td>69</td>\n",
       "      <td>228</td>\n",
       "      <td>280</td>\n",
       "      <td>545</td>\n",
       "      <td>219</td>\n",
       "      <td>263</td>\n",
       "      <td>526</td>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.693(0.0)</td>\n",
       "      <td>0.627(0.0)</td>\n",
       "      <td>0.622(-0.019)</td>\n",
       "      <td>0.629(0.014)</td>\n",
       "      <td>0.597(-0.063)</td>\n",
       "      <td>0.609(0.018)</td>\n",
       "      <td>0.731(0.001)</td>\n",
       "      <td>0.697(0.008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.689(0.0)</td>\n",
       "      <td>0.661(0.0)</td>\n",
       "      <td>0.579(-0.062)</td>\n",
       "      <td>0.586(-0.062)</td>\n",
       "      <td>0.486(-0.089)</td>\n",
       "      <td>0.614(0.035)</td>\n",
       "      <td>1.0(0.6)</td>\n",
       "      <td>0.76(0.028)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.945(0.0)</td>\n",
       "      <td>0.811(0.0)</td>\n",
       "      <td>0.693(0.17)</td>\n",
       "      <td>0.319(0.187)</td>\n",
       "      <td>0.711(0.244)</td>\n",
       "      <td>0.752(-0.149)</td>\n",
       "      <td>0.002(-0.002)</td>\n",
       "      <td>0.191(0.022)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-Score</th>\n",
       "      <td>0.797(0.0)</td>\n",
       "      <td>0.728(0.0)</td>\n",
       "      <td>0.631(0.055)</td>\n",
       "      <td>0.413(0.195)</td>\n",
       "      <td>0.577(0.062)</td>\n",
       "      <td>0.676(-0.029)</td>\n",
       "      <td>0.004(-0.004)</td>\n",
       "      <td>0.305(0.031)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Anger Anticipation        Disgust           Fear  \\\n",
       "True Positives         1176          977            632            255   \n",
       "True Negatives          180          250            584            976   \n",
       "False Positives         531          501            460            180   \n",
       "False Negatives          69          228            280            545   \n",
       "Accuracy         0.693(0.0)   0.627(0.0)  0.622(-0.019)   0.629(0.014)   \n",
       "Precision        0.689(0.0)   0.661(0.0)  0.579(-0.062)  0.586(-0.062)   \n",
       "Recall           0.945(0.0)   0.811(0.0)    0.693(0.17)   0.319(0.187)   \n",
       "F1-Score         0.797(0.0)   0.728(0.0)   0.631(0.055)   0.413(0.195)   \n",
       "\n",
       "                           Joy        Sadness       Surprise         Trust  \n",
       "True Positives             538            798              1           130  \n",
       "True Negatives             629            393           1429          1234  \n",
       "False Positives            570            502              0            41  \n",
       "False Negatives            219            263            526           551  \n",
       "Accuracy         0.597(-0.063)   0.609(0.018)   0.731(0.001)  0.697(0.008)  \n",
       "Precision        0.486(-0.089)   0.614(0.035)       1.0(0.6)   0.76(0.028)  \n",
       "Recall            0.711(0.244)  0.752(-0.149)  0.002(-0.002)  0.191(0.022)  \n",
       "F1-Score          0.577(0.062)  0.676(-0.029)  0.004(-0.004)  0.305(0.031)  "
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.printScoresAsTableWithDeltas(origScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta accuracy= [ 0.     0.    -0.019  0.014 -0.063  0.018  0.001  0.008]\n",
      "delta precision= [ 0.     0.    -0.062 -0.062 -0.089  0.035  0.6    0.028]\n",
      "delta recall= [ 0.     0.     0.17   0.187  0.244 -0.149 -0.002  0.022]\n",
      "delta f1score= [ 0.     0.     0.055  0.195  0.062 -0.029 -0.004  0.031]\n",
      "\n",
      "Change in f1-score:  0.478 -> 0.52  =  0.039\n"
     ]
    }
   ],
   "source": [
    "scores.printDeltas(origScores)\n",
    "\n",
    "(a, p, r, f1) = scores.getStats()\n",
    "\n",
    "print\n",
    "print \"Change in f1-score: \", np.round(mean_orig_f1,3), \"->\", np.round(np.mean(f1),2), \" = \", np.round((np.mean(f1)-mean_orig_f1), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above results it appears that we managed to increase the scores in all classes except anticipation which dropped .5% in accuracy. Clearly this strategy, while, on the whole, increasing the predictive performance of th model is not optimal. A perfect method would be to go through th entire search space of the problem, but for pairwise matching of emotions that would mean (8x8x4)! possible configurations, but this is not tractable.\n",
    "\n",
    "A possible better method to search through this state space would possibly be a genetic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 0.]\n",
      " [2. 0. 5.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "t = np.zeros((2, 3))\n",
    "t[0,0]=1\n",
    "t[1,0]=2\n",
    "t[0,1]=2\n",
    "t[1,2]=5\n",
    "print(t)\n",
    "\n",
    "#print(len(t))\n",
    "#t2 = np.sum(t, axis=1)\n",
    "#print(t2)\n",
    "#t2/sum(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 2., 5.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 1.        , 0.        ],\n",
       "       [1.        , 0.        , 1.66666667]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = [2, 2, 3]\n",
    "\n",
    "t/t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 5.],\n",
       "       [1., 2., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(t)-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., 10.,  0.],\n",
       "       [ 2.,  0.,  5.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t\n",
    "t[0,1] = 10\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(t)/t.shape[1], np.argmax(t) % t.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
